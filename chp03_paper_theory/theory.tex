\chapter{Characterising SDE linearisations: the theory}

\section{An explicit error bound}\label{sec:theory}
Suppose we are interested in the evolution of a \(\R^n\)-valued state variable \(y_t\) over a finite time interval \([0,T]\).
Our model, accounting for uncertainties arising from a range of sources, for the evolution of this variable is the It\^o stochastic differential equation
\begin{equation}
	\dif y_t^{(\epsilon)} = u\!\left(y_t^{(\epsilon)}, t\right)\dif t + \epsilon \, \sigma\!\left(y_t^{(\epsilon)}, t\right)\dif W_t,
	\label{eqn:sde_y}
\end{equation}
where \(u: \R^n \times [0,T] \to \R^n\) is the governing reference vector field, and can be inferred from underlying physics or available data, for instance.
The canonical \(m\)-dimensional Wiener process \(W_t\)  is a continuous white-noise stochastic process with independent Gaussian increments.
The scale of the ongoing noise is parameterised as \(0 < \epsilon \ll 1\) and \(\sigma: \R^n \times [0,T] \to \R^{n\times m}\) is a deterministic diffusion matrix.
The noise in \eqref{eqn:sde_y} is multiplicative, in that the diffusion matrix \(\sigma\) can vary with both state and time.
We assume that \(\sigma\) is specified \textit{a priori}, or if no such information is known, then \(\sigma \equiv I\), the \(n \times m\) identity matrix, is a default choice.

In the absence of any uncertainty (i.e. \(\epsilon = 0\)), \eqref{eqn:sde_y} reduces to the ordinary differential equation
\begin{equation}
	\dod{y_t^{(0)}}{t} = u\!\left(y_t^{(0)}, t\right).
	\label{eqn:ode_det}
\end{equation}
The formal convergence of the stochastic solution \(y_t^{(\epsilon)}\) to the deterministic \(y_{t}^{(0)}\) in the limit as \(\epsilon \to 0\) is well-established using large deviations principle, e.g. \cite{FreidlinWentzell_1998_RandomPerturbationsDynamical}.
Let the flow map \(F_{0}^{t}\colon \R^n \to \R^n\) be the function which evolves an initial condition from time \(0\) to time \(t\) according to the flow of \eqref{eqn:ode_det}.
We refer to \eqref{eqn:ode_det} as the \emph{reference} deterministic model associated with \eqref{eqn:sde_y} in that
it either demonstrates the dominant physics (as would be the case if we think of the noise in \eqref{eqn:sde_y} as
capturing stochastic parameterisation) or is the best-available model (for example if $ u $ is available from data, and \eqref{eqn:sde_y} represents the uncertainty of such data).
Solutions to the reference deterministic model are more readily available, e.g. in terms of computational efficiency when solving numerically, than those of the stochastic model, but do not account for inevitable uncertainty.

We start by considering \eqref{eqn:sde_y} subject to the \emph{general} uncertain initial condition \(y_0^{(\epsilon)} = x\), where \(x\) is an \(n\)-dimensional random vector with some given distribution.
We assume certain smoothness and boundedness conditions on the various terms outlined, which are stated explicitly in \Cref{hyp:smooth}.
Throughout, we use the norm symbol \(\norm{\cdot}\) to denote (i) for a vector, the standard Euclidean vector norm, (ii) for a matrix, the spectral norm induced by the Euclidean norm, and (iii) for a 3rd-order tensor, the spectral norm induced by the matrix norm.
The gradient symbol \(\nabla\) generically refers to derivatives with respect to the state variable.

\renewcommand\thehypo{H}
\begin{hypo}\label{hyp:smooth}
	Let the deterministic coefficients \(u \colon \R^n\times [0,T] \to \R^n\) and \(\sigma \colon \R^n \times [0,T] \to \R^{n\times m}\), and the random initial condition \(x\) be such that:
	\begin{enumerate}[label=(H.\arabic{*}), ref=H.\arabic{*}]
		\item\label{hyp:coef_cont} For each \(t \in [0,T]\), the function \(u(\cdot, t): \R^n \to \R^n\) given by \(u(x,t)\) is twice continuously differentiable on \(\R^n\), and each component of the function \(\sigma(\cdot, t): \R^n \to \R^{n\times m}\) given by \(\sigma(x,t)\) is differentiable on \(\R^n\).

		\item\label{hyp:u_bounds} There exists a constant \(K_{\nabla u} \geq 0\) such that for any \(t \in [0,T]\) and \(x \in \R^n\),
		\begin{equation*}
			\norm{\nabla u(x,t)} \leq K_{\nabla u}.
		\end{equation*}
		Equivalently, for all \(t \in [0,T]\), the function \(u\!\left(\cdot, t\right)\) is Lipschitz continuous with Lipschitz constant \(K_{\nabla u}\).

		\item\label{hyp:coef_meas} For each \(x \in \R^n\), the function \(u(x,\cdot) \coloneqq [0,T] \to \R^n\) and each component of the function \(\sigma(x,\cdot) \coloneqq [0,T] \to \R^{n\times m}\) are Borel-measurable on \([0,T]\).

		\item\label{hyp:linear_growth} There exists a constant \(K_L\) such that for any \(t \in [0,T]\) and \(x \in \R^n\),
		\[
			\norm{u\left(x,t\right)} + \norm{\sigma\left(x,t\right)} \leq K_L\left(1 + \norm{x}\right).
		\]

		\item\label{hyp:sigma_deriv_bound} There exists a constant \(K_{\nabla\sigma} \geq 0\) such that for any \(t \in [0,T]\) and \(x \in \R^n\),
		\begin{equation*}
			\norm{\nabla\sigma(x,t)} \leq K_{\nabla\sigma},
		\end{equation*}
		and we take \(K_{\nabla\sigma} = 0\) if there is no spatial dependence in \(\sigma\).
		Equivalently, for all \(t \in [0,T]\), the function \(\sigma\!\left(x, \cdot\right)\) is Lipschitz continuous with Lipschitz constant \(K_{\nabla\sigma}\).

		\item\label{hyp:init_indep} The initial condition \(x\) is defined on the same probability space as \(W_t\), and is independent of \(W_t\) for all \(t \in [0,T]\).

		\item\label{hyp:nnu_bounds} There exists a constant \(K_{\nabla\nabla u} \geq 0\) such that for any \(t \in [0,T]\) and \(x \in \R^n\),
		\[
			\norm{\nabla \nabla u(x,t)} \leq K_{\nabla\nabla u},
		\]
		and we take \(K_{\nabla\nabla} = 0\) if the second spatial derivatives of \(u\) are all zero.

		\item\label{hyp:sigma_bounds} There exists\lb{This assumption has been bothering me for a while; it is rather strong, but is only needed for a single step in the proof. There might be a way to drop it by using properties of a strong solution to an SDE, but that is so buried in stochastic process theory that I've not been able to work it out.} a constant \(K_\sigma \geq 0\) such that for any \(t \in [0,T]\) and \(x \in \R^n\),
		\begin{equation*}
			\norm{\sigma(x,t)} \leq K_{\sigma}.
		\end{equation*}

	\end{enumerate}
\end{hypo}
The conditions \ref{hyp:coef_cont} and \ref{hyp:u_bounds} ensure the existence of an invertible flow map \(F_s^t\) solving \eqref{eqn:ode_det}, and the addition of \ref{hyp:coef_meas} to \ref{hyp:init_indep} guarantee that \eqref{eqn:sde_y} with the initial condition \(y_0 = x\) has a unique strong solution \cite{KallianpurSundar_2014_StochasticAnalysisDiffusion}.
The bound \(K_{\nabla\nabla u}\) places on the second derivatives of \(u\) quantify exactly when the deterministic dynamics (that is, \(u\)) of \eqref{eqn:sde_y} are linear.
Similarly, the bound \(K_{\nabla\sigma}\) on the spatial derivatives of \(\sigma\) allows us to distinguish when the noise in \eqref{eqn:sde_y} is multiplicative.
% Only the bound on the diffusion coefficient \(\sigma\) in \ref{hyp:sigma_bounds} and the existence of bounded second derivatives of \(u\) in \ref{hyp:u_bounds} differ from the usual requirements for existence and uniqueness of solutions to stochastic differential equations of the form \eqref{eqn:sde_y}.

Our aim is to construct and formally justify a computable linearisation of \eqref{eqn:sde_y} about a trajectory solving the deterministic system \eqref{eqn:ode_det}.
To that end, we take a \emph{fixed} initial condition \(x_0 \in \R^n\) to the reference deterministic model \eqref{eqn:ode_det} and consider linearising the SDE \eqref{eqn:sde_y} about the corresponding trajectory \(F_0^t\!\left(x_0\right)\).
% The point \(x_0\) should be chosen to be an ``appropriate'' point within the distribution of \(x\), which we shall discuss in more formal terms later.
% We will provide bounds on the pathwise\lb{double check this is the correct phrase} error between the original SDE and the linearisation, written explicitly in terms of the noise scale \(\epsilon\) and the expected deviation between the reference point \(x\) and the initial condition \(\xi\).
% We further provide explicit expressions for the distribution of the linearised solution and computation of the first two moments, written in terms of only the initial distribution, the flow map of the deterministic system and \(\sigma\).
We consider the following linearisation of \eqref{eqn:sde_y};
\begin{equation}
	\dif l_t^{(\epsilon)} = \left[u\!\left(F_0^t\!\left(x_0\right), t\right) + \nabla u\!\left(F_0^t\!\left(x_0\right), t\right)\left(l_t^{(\epsilon)} - F_0^t\!\left(x_0\right)\right)\right]\dif t + \epsilon\sigma\!\left(F_0^t\!\left(x_0\right), t\right)\dif t, \quad l_0 = x.
	\label{eqn:linear_sde_inform}
\end{equation}
Informally, we can arrive at \eqref{eqn:linear_sde_inform} by performing a Taylor expansion of the coefficient \(u\) up to first-order and \(\sigma\) to zeroth-order about the time-varying trajectory \(F_0^t\!\left(x_0\right)\).
Such a linearisation is advantageous over the nonlinear SDE \eqref{eqn:sde_y}, since \eqref{eqn:linear_sde_inform} can be solved analytically.
We will later (see \Cref{thm:limit_sol} and \Cref{thm:limit_moments}) provide explicit expressions for computing the distribution of the solution \(l_t^{(\epsilon)}\) solely in terms of the solution dynamics of the deterministic system \eqref{eqn:ode_det}, and the distribution of \(x\).

% To express our results precisely in terms of convergences, we define the noise-scaled deviation
% \begin{equation}
% 	z_t^{(\epsilon)}\!\left(x, x_0\right) \coloneqq \frac{y_t^{(\epsilon)} - F_0^t(x_0)}{\epsilon} \, , \quad z_0^{(\epsilon)}\!\left(x, x_0\right) = \frac{x - x_0}{\epsilon},
% 	\label{eqn:z_def}
% \end{equation}
% where \(x_0 \in \R^n\) is fixed and certain, and \(y_0^{(\epsilon)} = x\).
% Equivalently, we are considering the first-order term in a power series expansion of the stochastic solution \(y_t^{(\epsilon)}\) in \(\epsilon\), i.e.
% \[
% 	y_t^{(\epsilon)} = F_0^t(x_0) + \epsilon z_t^{(\epsilon)}\!\left(x, x_0\right).
% \]
% All stochasticity in the solution \(y_t^{(\epsilon)}\), including the initial condition, is captured in the deviation \(z_t^{(\epsilon)}\), which is a convenient formulation for deriving computable expressions, as we shall see.
% \td{Emphasise this point later on.}
% We wish to understand the limiting behaviour of \(z_t^{(\epsilon)}\left(\xi, x\right)\) as \(\epsilon\) approaches zero, that is, in the limit of small noise.
% As it turns out, \(z_t^{(\epsilon)}\left(\xi, x\right)\) converges towards the solution of a linearised stochastic differential equation, which provides a computational characterisation and approximation for the deviation, and therefore the original stochastic process \(y_t^{(\epsilon)}\).
% Our first and primary result, \Cref{thm:main}, rigorously establishes an explicit bound on the pathwise error between \(z_t^{(\epsilon)}(x)\) and a linearisation about the deterministic trajectory starting at \(x\).

In order to quantify the error arising from the choice of reference point \(x_0\), we define
\[
	\delta_r \coloneqq \avg{\norm{x - x_0}^r}^{1/r},
\]
i.e. \(\delta_r\) is the \(L^r\) distance between \(x\) and the deterministic point \(x_0\).
We can think of \(\delta_r\) as a scalar measure of the uncertainty in the initial condition, relative to the choice of reference point \(x_0\).
Alternatively, the limit as \(\delta_r\) approaches zero is equivalent to convergence of \(r\)th mean of \(x\) to the fixed point \(x_0\).
We can therefore distinguish two sources of uncertainty in our model; that arising from the initial condition, quantified by \(\delta_r\), and the ongoing uncertainty driven by the Wiener process \(W_t\) and measured by \(\epsilon\).

Our first and primary result, \Cref{thm:main}, provides an explicit bound on the \(r\)th moment of the error between the SDE solution \(y_t^{(\epsilon)}\) and the linearised solution \(l_t^{(\epsilon)}\).

\begin{theorem}[Linearisation error is bounded]\label{thm:main}
	Let \(y_t^{(\epsilon)}\) be the strong solution to the SDE \eqref{eqn:sde_y} and \(l_t^{(\epsilon)}\) be the strong solution to the corresponding linearisation \eqref{eqn:linear_sde_inform}, both driven by the \emph{same} Wiener process \(W_t\) and subject to the same random initial condition \(y_0^{(\epsilon)} = l_0^{(\epsilon)} = x\).
	Then, for any \(r \geq 1\) such that \(\delta_{2r} < \infty\) and \(t \in [0,T]\), there exist constants \( D_1(r,t), D_2(r,t), D_3(r,t) \in [0, \infty) \) independent of \(x\) and \(x_0\) such that for all \(\epsilon > 0\),
	\begin{equation}
		\avg{\norm{y_t^{(\epsilon)} - l_t^{(\epsilon)}}^r} \leq \begin{multlined}[t]
			\left(K_{\nabla\nabla u}^r + K_{\nabla\sigma}^r\right) D_1(r,t)\, \epsilon^{2r} + K_{\nabla\nabla u}^r D_2(r,t)\delta_{2r}^{2r}
			+ K_{\nabla\sigma}^r D_3(r,t)\delta_r^r \epsilon^r.
		\end{multlined}
		\label{eqn:main_ineq}
	\end{equation}
\end{theorem}
\begin{proof}
	See \Cref{app:main_thm_proof}.
	Our proof employs the Burkholder-Davis-Gundy inequality, Gr\"onwall's inequality, and Taylor's theorem to explicitly construct the bounding coefficients in terms of the conditions on the SDE coefficients set out in \Cref{hyp:smooth}.
	The bounding coefficients \(D_1\), \(D_2\), and \(D_3\) are given explicit in \eqref{eqn:bound_defns}.
\end{proof}
In \eqref{eqn:main_ineq}, we have an explicit scaling of the error in terms of \(\epsilon\) and \(\delta_r\).
The three terms can be loosely\lb{Is this too informal?} thought of as: a contribution purely from the ongoing linearisation error, a contribution purely from the initial uncertainty, and a term resulting from the interaction between the initial and ongoing uncertainties.
By explicitly identifying the dependence of the bound on \(K_{\nabla\nabla u}\) and \(K_{\nabla \sigma}\), we note three special cases that are summarised by \Crefrange{rem:bound_linear}{rem:bound_exact}.

\begin{remark}[Linear drift]\label{rem:bound_linear}
	When the deterministic dynamics are linear, we set \(K_{\nabla\nabla u} = 0\) and \eqref{eqn:main_ineq} becomes
	\[
		\avg{\norm{y_t^{(\epsilon)} - l_t^{(\epsilon)}}^r} \leq   K_{\nabla\sigma}^r D_1(r,t)\, \epsilon^{2r} + K_{\nabla\sigma}^r D_3(r,t)\delta_r^r \epsilon^r.
	\]
	The linearisation of the drift term \(u\) is exact, so the error is purely due to the spatial dependency of the diffusion term \(\sigma\).
\end{remark}

\begin{remark}[Additive noise]\label{rem:bound_additive}
	When the noise in \eqref{eqn:sde_y} is additive (non-multiplicative), we set \(K_{\nabla\sigma} = 0\) and \eqref{eqn:main_ineq} becomes
	\[
		\avg{\norm{y_t^{(\epsilon)} - l_t^{(\epsilon)}}^r} \leq   K_{\nabla\nabla u}^r D_1(r,t)\, \epsilon^{2r} + K_{\nabla\nabla u}^r D_2(r,t)\delta_{2r}^{2r}.
	\]
	The error is then purely due to the linearisation of the drift term \(u\), and as expected is of second order in both the initial condition uncertainty \(\delta_{2r}\) and the ongoing uncertainty \(\epsilon\).
\end{remark}

\begin{remark}[Exact linearisation]\label{rem:bound_exact}
	When the deterministic dynamics are linear and the noise in \eqref{eqn:sde_y} is additive (non-multiplicative), the linearisation \eqref{eqn:linear_sde_inform} should be exact.
	Accordingly, we set \(K_{\nabla\nabla u} = K_{\nabla\sigma} = 0\) and \eqref{eqn:main_ineq} becomes
	\[
		\avg{\norm{y_t^{(\epsilon)} - l_t^{(\epsilon)}}^r} = 0.
	\]
	In turn, this implies that \(y_t^{(\epsilon)} = l_t^{(\epsilon)}\) almost surely, for \emph{any} choice of reference point \(x_0\).
\end{remark}

% In \eqref{eqn:main_ineq}, we have a condition on the reference point \(x\) relative to the initial condition \(\xi\) that ensures that the linearisation approximation is justified.
% \td{Elaborate in more detail about the implied conditions on the initial condition.}
% In the limit of small ongoing noise (\(\epsilon \to 0\)), provided that \(\delta_I\!\left(2r\right)\) approaches zero, \eqref{eqn:main_ineq} implies that \(z_t^{(\epsilon)}\!\left(x,x_0\right)\) converges in \(r\)th moment to the linearised solution \(z_t\!\left(x, x_0\right)\), which in turn implies convergence in probability.
% \td{A rate of convergence? Markov's inequality means that through \eqref{eqn:main_ineq_fixed} we can bound the probability that the SDE solution is a given distance away from the linearised one.}
% Taking the limit as \(\epsilon\) approaches 0 in \eqref{eqn:main_ineq} shows that \(z_t^{(\epsilon)}(x)\) converges in \(r\)th moment to \(z_t(x)\), which in turn implies convergence in probability and convergence in distribution (or weak convergence).

% It is important to note that the stochastic differential equation \eqref{eqn:sde_y} and the linearised equation \eqref{eqn:limit_sde} must be defined with the \emph{same} Wiener process \(W_t\) for \Cref{thm:main} to hold as stated.
% \rev{This distinction is exactly the difference between strong and weak solutions to a stochastic differential equation; a strong solution requires knowledge of the underlying Wiener process, whereas a weak solution only requires a Wiener process following the same distribution.}
% By weakening the notion of convergence we can think of \(z_t^{(\epsilon)}(x)\) as converging to a Gaussian distribution (the distribution of the linearised solution) with no reference to the driving Wiener process.

We next explicitly establish the distribution of the solution to the linearisation \eqref{eqn:linear_sde_inform}, in terms of the initial condition and the deterministic evolution of \eqref{eqn:ode_det}.
The solution is an independent sum of a linear transformation of the initial condition and a Gaussian random variable, which is a convenient expression for both theoretical analysis and numerical computation.

\begin{theorem}[Distribution of the linearised solution]\label{thm:limit_sol}
	The strong solution to the linearised SDE \eqref{eqn:linear_sde_inform} is
	\begin{equation}
		l_t^{(\epsilon)} = \nabla F_0^t\!\left(x_0\right)\left(x - x_0\right) + F_0^t\!\left(x_0\right) + \epsilon\nabla F_0^t\!\left(x_0\right)\int_0^t{L\!\left(x_0, \tau\right)\dif W_\tau}.
		\label{eqn:linear_sol}
	\end{equation}
	where the two random summands are independent, and
	\begin{equation}
		L\!\left(x_0, \tau\right) \coloneqq \left[\nabla F_0^\tau(x_0)\right]^{-1}\sigma\left(F_0^\tau(x_0), \tau\right).
		\label{eqn:sigma_L_def}
	\end{equation}
	Furthermore,
	\[
		\int_0^t{L\!\left(x_0,\tau\right)\dif W_\tau} \isGauss{0, \int_0^t{L\!\left(x_0, \tau\right)L\!\left(x_0, \tau\right)^{\T}\dif\tau}},
	\]
	independently of \(x\).
	% where \(\Sigma_0^t\!\left(x_0\right)\) is an \(n \times n\) positive-definite matrix defined as
	% \begin{equation}
	% 	\Sigma_0^t\!\left(x_0\right) = \int_0^t{L\left(x_0, t, \tau\right)L\left(x_0, t, \tau\right)^{\T}\dif\tau},
	% 	\label{eqn:sigma_def}
	% \end{equation}
\end{theorem}
\begin{proof}
	See \Cref{app:limit_sol_proof}.
\end{proof}

The representation of the linearised solution as an independent sum in \eqref{eqn:linear_sol} can be seen as a decomposition into the initial uncertainty (the transformation of initial condition \(x\)), a deterministic prediction (the flow map \(F_0^t\!\left(x_0\right)\)) and the ongoing uncertainty in \(u\) (the remaining It\^o integral term).
% \td{Justify why we've explicitly stated the distribution of the Ito integral.}
% The covariance matrix \(\Sigma_0^t\!\left(\right)\) of the It\^o integral term captures purely the ongoing uncertainty.
% We have provided an explicit expression for \(\Sigma_0^t\!\left(x_0\right)\) in \eqref{eqn:sigma_def}, written entirely in terms of the deterministic system \eqref{eqn:ode_det} and specification of \(\sigma\).
% As an alternative, \(\Sigma_0^t\!\left(x_0\right)\) is the positive semi-definite solution to
% \[
% 	\dod{\Sigma_0^t\!\left(x_0\right)}{t} = \left[\nabla u\!\left(F_0^t\!\left(x_0\right), t\right)\right]\Sigma_0^t\!\left(x_0\right) + \Sigma_0^t\!\left(x_0\right) \left[\nabla u\!\left(F_0^t\!\left(x_0\right), t\right)\right]^{\T} + \sigma\!\left(F_0^t\!\left(x_0\right), t\right)\sigma\!\left(F_0^t\!\left(x_0\right)\right),
% \]
% subject to \(\Sigma_0^0\!\left(x_0\right) = O\), the \(n \times n\) zero matrix.

% For small \(\epsilon\) and initial uncertainty, we have therefore justified the approximation
% \begin{equation}\label{eqn:y_t_approx}
% 	y_t^{(\epsilon)} \stackrel{.}{\sim} \nabla F_0^t(x) \left(\xi - x\right) + \left(F_0^t(x) + \epsilon \zeta_0^t(x)\right).
% \end{equation}
% We can also think of \eqref{eqn:y_t_approx} as an approximate decomposition of the SDE solution into an independent sum of stochasticity due to the initial condition and that introduced by the ongoing uncertainty \(W_t\).
% Such a representation is convenient for both analytical manipulation and when simulating numerically.
% For example, if \(x\) has probability density function \(p_{x}\), then the probability density function \(p_z\) at time \(t\) of the limiting solution \(z_t\!\left(x, x_0\right)\) is the convolution
% \[
% 	p_z\!\left(z,t\right) = \frac{1}{\det{\nabla F_0^t\!\left(x_0\right)}}\int_{\R^n}{p_x\!\left(\left[\nabla F_0^t\!\left(x_0\right)\right]^{-1} \eta + x\right) \mathcal{N}\!\left(z - \eta; 0, \Sigma_0^t\!\left(x_0\right)\right)\dif \eta},
% \]
% where \(\mathcal{N}\!\left(\cdot; 0, \Sigma_0^t\!\left(x_0\right)\right)\) denotes the PDF of the \(n\)-dimensional Gaussian distribution with mean \(0\) and covariance matrix \(\Sigma_0^t\!\left(x_0\right)\).
Finally, we establish explicit expressions for the mean and covariance of the limiting solution, and ordinary differential equations for both moments.
% These expressions are consistent with those used across a diversity of literature for linearisation \cite{Jazwinski_2014_StochasticProcessesFiltering} and Gaussian approximations of stochastic differential equation solutions \cite{SarkkaSolin_2019_AppliedStochasticDifferential}.

\begin{corollary}[Computation of the limiting mean and variance]\label{thm:limit_moments}
	For any \(x \in \R^n\) and \(t \in [0,T]\), the mean of the linearised solution can be written as
	\begin{equation}
		\avg{l_t^{(\epsilon)}} = F_0^t\!\left(x_0\right) + \nabla F_0^t\!\left(x_0\right) \left(\avg{x} - x_0\right).
		\label{eqn:mean_expl_eqn}
	\end{equation}
	% and is the solution to the ordinary differential equation
	% \begin{equation}
	% 	\dod{\mu(t)}{t} = u\!\left(F_0^t\!\left(x_0\right), t\right) + \nabla u\!\left(F_0^t\!\left(x_0\right), t\right) \mu(t),
	% 	\label{eqn:mean_ode}
	% \end{equation}
	% subject to the initial condition \(\mu(0) = \avg{x}\).
	% and is the solution to the equation of variations corresponding to \eqref{eqn:ode_det}, i.e.
	% \td{Variation of parameters!}
	% \begin{equation}
	% 	\dod{\mu_t\!\left(x, x_0\right)}{t} = \nabla u\!\left(F_0^t\!\left(x_0\right), t\right)\mu_t\!\left(x, x_0\right)
	% 	\label{eqn:mean_ode}
	% \end{equation}
	% subject to \(\mu_0\!\left(x, x_0\right) = \avg{x} - x_0\).

	The \(n\times n\) covariance matrix of the linearised solution is given explicitly by
	\begin{equation}
		\var{l_t^{(\epsilon)}} = \nabla F_0^t\!\left(x_0\right)\left(\var{x} + \int_0^t{L\!\left(x_0,\tau\right)L\!\left(x_0,\tau\right)^{\T}\dif\tau}\right)\left[\nabla F_0^t\!\left(x_0\right)\right]^{\T}
		\label{eqn:pi_expl_eqn}
	\end{equation}
	where \(L\!\left(x_0, \tau\right)\) is as defined in \eqref{eqn:sigma_L_def} and the integral is taken in the elementwise sense.
	Additionally, \(\var{l_t^{(\epsilon)}}\) is the symmetric positive-semidefinite \(n \times n\) matrix solution to the ordinary differential equation
	\begin{equation}
		\dod{\Pi(t)}{t} = \begin{multlined}[t]
			\nabla u\!\left(F_0^t\!\left(x_0\right), t\right) \Pi(t) + \Pi(t)\left[\nabla u\!\left(F_0^t\!\left(x_0\right), t\right)\right]^{\T} + \epsilon^2\sigma\!\left(F_0^t\!\left(x_0\right), t\right)\sigma\!\left(F_0^t\!\left(x_0\right), t\right)^{\T},
		\end{multlined}
		\label{eqn:pi_ode}
	\end{equation}
	subject to the initial condition \(\Pi(0) = \var{x}\).
\end{corollary}
\begin{proof}
	See \Cref{app:limit_moments_proof}.
	The expressions follow from the representation of the linearised solution as an independent sum in \eqref{eqn:linear_sol}.
\end{proof}
In \Cref{thm:limit_sol} and \Cref{thm:limit_moments}, we have provided expressions for the distribution of the solution \(l_t^{(\epsilon)}\) to the linearised SDE \eqref{eqn:linear_sde_inform} written in terms of solely the solution behaviour of the deterministic system \eqref{eqn:ode_det}, the specified diffusion matrix \(\sigma\), and the distribution of the initial condition \(x\).
This describes a method for approximating the solution to the nonlinear SDE \eqref{eqn:sde_y}, or equivalently characterising the impact of uncertainty in a dynamical system \eqref{eqn:ode_det}, that circumvents the need for expensive stochastic simulation.

% In particular, the differential equation \eqref{eqn:pi_ode} for the covariance matrix matches that seen in other literature on the propagation of uncertainty through a linearised system \cite{Sanz-AlonsoStuart_2017_GaussianApproximationsSmall,Jazwinski_2014_StochasticProcessesFiltering,SarkkaSolin_2019_AppliedStochasticDifferential,ArchambeauEtAl_2007_GaussianProcessApproximations}.

We briefly consider two special cases for the initial condition \(x\), a fixed (deterministic) initial condition, and a Gaussian initial condition.
In both these cases, the linearised solution also follows a Gaussian distribution which are characterised entirely by the mean and covariance described in \Cref{thm:limit_moments}, allowing for easy computation.
% \td{Motivate why we care about these two cases. Fixed initial condition; quantifying uncertainty purely arising from models, stochastic sensitivity. Gaussian initial condition; iterative filtering schemes, Gaussian assumed approximations?}
We also relate these results directly to other literature \cite{Jazwinski_2014_StochasticProcessesFiltering,FreidlinWentzell_1998_RandomPerturbationsDynamical,Blagoveshchenskii_1962_DiffusionProcessesDepending,Balasuriya_2020_StochasticSensitivityComputable,Sanz-AlonsoStuart_2017_GaussianApproximationsSmall,SarkkaSolin_2019_AppliedStochasticDifferential} which uses linearisation procedures and Gaussian process approximations for nonlinear SDEs in these situations.\lb{I probably need to be more specific here. The fixed initial condition case is specific to the small-noise papers, and has an application in stochastic sensitivity (as we show later). The Gaussian initial condition is useful in iterative schemes (right??) and Gaussian-assumed approximations.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fixed initial condition}\label{sec:theory_fixed}
Consider when the initial condition \(x\) is itself a fixed and known deterministic value, in which case we take \(x_0 = x\) and \(\delta_r = 0\) for all \(r\).
In this situation, the bound \eqref{eqn:main_ineq} on the linearisation error reduces to
\begin{equation}
	\avg{\norm{y_t^{(\epsilon)} - l_t^{(\epsilon}}^r} \leq \left(K_{\nabla\nabla u}^r + K_{\nabla\sigma}\right)D_1(r,t)\epsilon^{2r}.
	\label{eqn:main_ineq_fixed}
\end{equation}
We can consider the linearisation as equivalently arising from a first-order power series expansion of \(y_t^{(\epsilon)}\) in the noise-scale parameter \(\epsilon\), i.e.
\[
	y_t^{(\epsilon)} = F_0^t\!\left(x_0\right) + \epsilon z_t^{(\epsilon)} + R_2\left(x,t,\epsilon\right).
\]
where \(z_\epsilon \coloneqq \left(l_{t}^{(\epsilon)} - F_0^t\!\left(x_0\right)\right) / \epsilon\) is the first order term and \(R_2\) is a random quantity capturing the remaining deviation between \(y_t^{(\epsilon)}\) and the linearisation.
By rearranging and taking \(r = 1\) in \eqref{eqn:main_ineq_fixed}, we therefore have the explicit Taylor-like bound
\[
	\frac{\avg{\norm{R_2\left(x,t,\epsilon\right)}}}{\epsilon^2} \leq \left(K_{\nabla \nabla u} + K_{\nabla\sigma}\right)D_1\!\left(1, t\right).
\]
This result is consistent with the formulation of the bounds by \citet{Blagoveshchenskii_1962_DiffusionProcessesDepending} and \citet{FreidlinWentzell_1998_RandomPerturbationsDynamical}, for instance.
Moreover, the distribution of the linearisation solution \eqref{eqn:linear_sol} is Gaussian, which through \Cref{thm:limit_moments} we can explicitly characterise in terms of the deterministic system, namely
\begin{equation}
	l_t^{(\epsilon)} \isGauss{F_0^t\!\left(x_0\right), \Sigma_0^t\!\left(x_0\right)},
	\label{eqn:linear_gauss_sol}
\end{equation}
where \(\Sigma_0^t\!\left(x_0\right)\) is given explicitly by
\begin{equation}\label{eqn:sigma_def}
	\Sigma_0^t\!\left(x_0\right) = \nabla F_0^t\!\left(x_0\right)\left(\int_0^t{L\!\left(x_0, \tau\right)L\!\left(x_0, \tau\right)\dif\tau}\right)\left[\nabla F_0^t\!\left(x_0\right)\right]^{\T},
\end{equation}
and is the solution to the matrix differential equation \eqref{eqn:pi_ode}, subject to \(\Sigma_0^0\!\left(x_0\right) = O\), the \(n \times n\) zero matrix.
The distribution can be computed \emph{entirely} from the solution behaviour of the deterministic equation \eqref{eqn:ode_det} and prior specification of \(\sigma\).
In \Cref{sec:theory_s2}, we demonstrate an application of these results to extend stochastic sensitivity \cite{Balasuriya_2020_StochasticSensitivityComputable} to arbitrary dimension.

% \begin{remark}
%     The probability density function of the Gaussian solution \eqref{eqn:linear_gauss_sol}
%     \[
%     p\!\left(z; x_0, t\right) = \left(2^{n}\pi^{n}\det\left[\Sigma_0^t\!\left(x_0\right)\right]\right)^{-1/2}\exp\left[-\frac12\left(z - F_0^t\!\left(x_0\right)\right)^{\T}\left[\Sigma_0^t\!\left(x_0\right)\right]^{-1} \left(z - F_0^t\!\left(x_0\right)\right)\right],
%     \]
%     is the fundamental solution (transition density function) of the Fokker-Planck equation \cite{Risken_2012_FokkerPlanckEquationMethods} corresponding to the linearised equation \eqref{eqn:linear_sde_inform}, i.e.
%     \[
%     \dpd{p\!\left(z; x_0, t\right)}{t} = -\nabla\cdot\left(p\!\left(z; x_0, t\right) \nabla u\!\left(F_0^t\!\left(x_0\right), t\right) z\right) + \frac{\epsilon^2}{2}\mathrm{tr}\!\left(\sigma\left(F_0^t\!\left(x_0\right), t\right)^{\T}\nabla\nabla p\!\left(z; x_0, t\right)\right).
%     \]
% \end{remark}

% An equivalent statement to \eqref{eqn:main_ineq_fixed} is
% \begin{equation}\label{eqn:main_ineq_y}
% 		\avg{\norm{y_t^{(\epsilon)} - \left(F_0^t(x) + \epsilon z_t(x)\right)}^r} \leq D_r(t)\, \epsilon^{2r}.
% \end{equation}
% 		We can consider the linearisation \(F_0^t(x) + \epsilon z_t(x)\) as a first-order series expansion of \(y_t^{(\epsilon)}\) in the noise-scale parameter \(\epsilon\), i.e.
% 		\[
% 			y_t = F_0^t(x) + \epsilon z_t(x) + R_2\left(x,t,\epsilon\right).
% 		\]
% 		By taking \(r = 1\) in \eqref{eqn:main_ineq_y}, we therefore have the explicit bound
% 		\[
% 			\frac{\avg{\norm{R_2\left(x,t,\epsilon\right)}}}{\epsilon^2} \leq D_1(t).
% 		\]
% Therefore, for a fixed time \(t \in [0,T]\), \Cref{thm:limit_sol} and \eqref{eqn:main_ineq_fixed} justifies the approximation
% \begin{equation}
% 	y_t^{\left(\epsilon\right)} \stackrel{.}{\sim} \Gauss{F_0^t\left(x\right), \epsilon^2 \Sigma_0^t\left(x\right)},
% 	\label{eqn:y_t_gauss}
% \end{equation}
% for small values of \(\epsilon\), in the sense of a small-noise limit and bounded error.
% \rev{In particular, \eqref{eqn:main_ineq_y} places a bound on both the strong and weak error, and therefore the respective convergences, of the linearisation approximation.}

% Beyond providing a now justified approximation for small-noise regimes, the Gaussian limit provides a characterisation of the stochastic system, by capturing the impact of \emph{both} the deterministic dynamics of the model and multiplicative noise in a manner that is computationally easy to compute.
% We explore this notion further by using this theory to extend the recently developed stochastic sensitivity \cite{Balasuriya_2020_StochasticSensitivityComputable}, a computable tool for characterising uncertainty, in the next section.


\subsection{Gaussian initial condition}
We now briefly consider the case when the initial condition follows a Gaussian random variable itself, for instance \(x \isGauss{\mu_0, \Sigma_0}\), where \(\mu_0 \in \R^n\) and \(\Sigma_0 \in \R^{n \times n}\) are fixed and specified.
The linearisation then follows a Gaussian distribution itself, which is entirely characterised by the mean and covariance matrix described in \Cref{thm:limit_moments}.
Alternatively, these moments can be conveniently computed by simultaneously solving \eqref{eqn:ode_det} for the state variable\lb{Possibly unclear - need \(\nabla F\) to compute the mean in the general case.} and \eqref{eqn:pi_ode} for the linearised covariance.

A natural choice of reference point \(x_0\) is the mean of the initial Gaussian density, i.e. \(x_0 = \mu_0\).
The \(L_r\) distance between \(x\) and the mean \(\mu\) can be bounded by the trace of \(\Sigma_0\); for example, one such bound is
\begin{equation}\label{eqn:gauss_dist_bound}
	\delta_r^{r} \leq n^{3r/2 - 1} M_r \mathrm{tr}\left(\Sigma_0\right)^{r/2}, \quad M_r \coloneqq \frac{2^{r/2}\Gamma\!\left(\frac{r + 1}{2}\right)}{\sqrt{\pi}},
\end{equation}
where \(\Gamma\) denotes the Gamma function.
The initial covariance \(\Sigma_0\) directly measures the uncertainty in the initial condition, and we see through \eqref{eqn:gauss_dist_bound} that as the components of \(\Sigma_0\) approach zero, the contribution of the initial uncertainty to the linearisation error in \eqref{eqn:main_ineq} approaches zero also.
The linearised solution is then
\[
	l_t^{(\epsilon)} \isGauss{F_0^t\!\left(x_0\right), \, \nabla F_0^t\!\left(x_0\right) \var{x}\left[\nabla F_0^t\!\left(x_0\right)\right]^{\T} + \varepsilon^2 \Sigma_0^t\!\left(x_0\right)}.,
\]
where the covariance matrix is also the solution to \eqref{eqn:pi_ode} subject to the initial condition \(\Pi(0) = \var{x}\).
By jointly solving \eqref{eqn:ode_det} for the deterministic trajectory (the mean of \(l_t^{(\epsilon)}\)) and \eqref{eqn:pi_ode} for the covariance matrix, one can easily compute the linearised solution, describing exactly the assumed Gaussian approximation presented in \citet{SarkkaSolin_2019_AppliedStochasticDifferential}, and the linearisation used in the extended Kalman filter \cite{Jazwinski_2014_StochasticProcessesFiltering} and considered by \citet{Sanz-AlonsoStuart_2017_GaussianApproximationsSmall}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extending stochastic sensitivity}\label{sec:theory_s2}
The results of \Cref{sec:theory_fixed} for a fixed initial condition provides a direct extension of the stochastic sensitivity tools first introduced by \citet{Balasuriya_2020_StochasticSensitivityComputable} for the fluid flow context.
Here, the deterministic model \eqref{eqn:ode_det} is seen as a ``best-available'' model for the evolution of trajectories, and the driving vector field \(u\) is the Eulerian velocity of the fluid.
Stochastic sensitivity ascribes a scalar value to each deterministic trajectory by computing the maximum variance of the scaled deviations, when projected onto rays emanating from the origin \cite{Balasuriya_2020_StochasticSensitivityComputable}.
The aim is to provide a \emph{single} computable number for each deterministic trajectory quantifying the impact of uncertainty in the velocity, independent of the scale (\(\epsilon\)) of the noise.
The natural restating of this original definition of stochastic sensitivity \cite{Balasuriya_2020_StochasticSensitivityComputable} in the $ n $-dimensional setting is as follows:

\begin{definition}[Stochastic sensitivity in \(\R^n\)]\label{def:ss_Rn}
	The \emph{stochastic sensitivity} is a scalar field \(S^2: \R^n \times [0,T] \to \left[0, \infty\right)\) given by
	\begin{equation*}
		S^2\!\left(x_0,t\right) \coloneqq \lim_{\epsilon\downarrow 0}\sup\set{\frac{1}{\epsilon}\var{p^{\T}\left(y_t^{(\epsilon)} - F_0^t\!\left(x_0\right)\right)} \,: \, p \in \R^n, \, \norm{p} = 1}.
	\end{equation*}
\end{definition}

\Cref{def:ss_Rn} is in the spirit of principal components analysis \cite{Jolliffe_2002_PrincipalComponentAnalysis}, performing a dimension reduction by projecting onto the direction in which the variance is maximised, thus capturing the most uncertainty in the data with a scalar value.
The anisotropic uncertainty in two-dimensions \cite{Balasuriya_2020_StochasticSensitivityComputable} is the direction-dependent projection (prior to optimising over all directions in \Cref{def:ss_Rn}).
Explicit theoretical expressions for both the stochastic sensitivity and the anisotropic sensitivity in two dimensions were obtained by \citet{Balasuriya_2020_StochasticSensitivityComputable}; these allowed for quantifying certainty in eventual trajectory locations without having to perform stochastic simulations.
We show here that our results in \(n\)-dimensions are a generalisation of the two-dimensional ones in \cite{Balasuriya_2020_StochasticSensitivityComputable}, which moreover establish Gaussianity as well as an explicit expression for the uncertainty measure.
A theoretically pleasing and computable expression for the stochastic sensitivity is obtainable;


\begin{theorem}[Computation of \(S^2\)]\label{thm:s2_calculation}
	For any \(x \in \R^n\) and \(t \in [0,T]\),
	\begin{equation}
		S^2\!\left(x_0,t\right) = \norm{\Sigma_0^t\!\left(x_0\right)},
		\label{eqn:s2_calculation}
	\end{equation}
	where the covariance matrix \(\Sigma_0^t\) is defined in \eqref{eqn:sigma_def}.
	Equivalently, \(S^2\!\left(x_0,t\right)\) is given by the maximum eigenvalue of \(\Sigma_0^t\!\left(x_0\right)\).
\end{theorem}
\begin{proof}
	See \Cref{app:s2_calculation_proof}.
	This result uses \Cref{thm:main} to establish the convergence of the covariance matrices, and then the properties of the spectral norm to establish \eqref{eqn:s2_calculation}.
\end{proof}
The stochastic sensitivity field can be calculated given any velocity data \(u\), and through the explicit expression \eqref{eqn:sigma_def} for \(\Sigma_0^t\) can even be computed from only flow map data.
Computation does not require knowledge of the noise scale \(\epsilon\), so the \(S^2\) field is intrinsic in capturing the impact of the model dynamics on uncertainty, and any specified non-uniform diffusivity.

% The definitions of resolution- and noise-scaled stochastic sensitivity and robust sets with respect to stochastic sensitivity introduced by Balasuriya \cite{Balasuriya_2020_StochasticSensitivityComputable} can be easily extended using the new Definition \ref{def:ss_Rn}.
It has already been shown that, in the fluid flow context, stochastic sensitivity can identify coherent regions in two-dimensions \cite{BadzaEtAl_2023_HowSensitiveAre, Balasuriya_2020_StochasticSensitivityComputable}.
A simple approach is to define robust sets, which are those initial conditions for which the corresponding \(S^2\) value, i.e., the uncertainty in eventual location,  are below some specified threshold.
This threshold can be defined precisely in terms of a spatial lengthscale of interest and the advective and diffusive characteristics of the flow, as Definition 2.9 of \cite{Balasuriya_2020_StochasticSensitivityComputable}.
Such a definition extends to the \(n\)-dimensional case as presented here, moreover establishing an easily computable method for determining coherent sets by using the covariance matrix \(\Sigma_0^t\).

% Independent of the fluid mechanics context, \Cref{thm:s2_calculation} indicates that even for general systems, the matrix norm
% of \(\Sigma_0^t(x)\), i.e., the stochastic sensitivity \(S^2(x,t)\), can be used as {\em one} number which encapsulates the uncertainty of an initial state \(x\) after \(t\) time units.

% We compare stochastic sensitivity to two alternative measures for uncertainty quantification in Lagrangian systems that have been developed recently.
% \citet{BranickiUda_2023_PathBasedDivergenceRates} build upon their earlier foundational work in \cite{BranickiUda_2021_LagrangianUncertaintyQuantification} to develop a framework for quantifying Lagrangian uncertainty via probabilistic divergences.
% \td{One or two lines here comparing with the Branicki stuff}
% \citet{KaszasHaller_2020_UniversalUpperEstimate} introduce ``model sensitivity'' as an upper bound on the impact of both deterministic and ongoing stochastic perturbations to a dynamical system.
% The development of this measure relies upon a linearisation approximation to derive explicit formulae, for which we have now provided an explicit bound on the error through \Cref{thm:main}.
% \td{Try to quickly explain how the two approaches are different, without talking down on the other. SS is purely stochastic, provides directional insight and the results are exact in a limit. MS accounts for \emph{deterministic} perturbations, rather than initial uncertainty, and is an upper bound that captures the magnitude of the deviation, rather than the direction as SS does.}
% On the other hand, the stochastic sensitivity approach provides insight into the \emph{directional} uncertainty.


Finally, we briefly consider the implications of our results on uncertain initial conditions from \Cref{sec:theory} on stochastic sensitivity.
Consider an uncertain initial condition \(x\) with expectation \(\avg{x} = x_0\) and variance \(\var{x} = \epsilon^2 I\), which is assuming an isotropic\lb{Might only be the case is the distribution is something ``symmetric'' like a Gaussian. You can easily have a whacky distribution with the same mean and variance.} uncertainty scaled by \(\epsilon\).
If there was no ongoing uncertainty (\(\sigma \equiv O\)), then any uncertainty in the system arises purely from the initial condition \(x\).
Then from \eqref{eqn:pi_expl_eqn}, the variance of the small noise linearisation is then
\[
	\var{l_t^{(\epsilon)}} = \nabla F_0^t\!\left(x_0\right)\left[\nabla F_0^t\!\left(x_0\right)\right]^{\T},
\]
which is the left Cauchy-Green tensor of the deterministic system \eqref{eqn:ode_det}.
By maximising the projection of this variance over all directions, as in \eqref{eqn:s2_calculation} to compute stochastic sensitivity, we perform exactly the computation to determine the maximal stretching rate in the computation of the finite-time Lyapunov exponent.
The finite-time Lyapunov exponent (FTLE) quantifies the sensitivity of a dynamical system to initial conditions \cite{ShaddenEtAl_2005_DefinitionPropertiesLagrangian}, and can be equivalently considered a measure of the impact of an uncertain initial condition on the \emph{deterministic} evolution of trajectories.
There has been recent interest in extending the FTLE for systems with ongoing uncertainty \cite{Balasuriya_2020_UncertaintyFinitetimeLyapunov,YouLeung_2021_ComputingFiniteTime,GuoEtAl_2016_FiniteTimeLyapunovExponents}, but no established approach as of yet.
A framework that computes stochastic sensitivity with uncertain initial conditions can be seen as such an extension of the FTLE, in the sense that the measure would characterise the sensitivity of a dynamical system to \emph{both} initial conditions and ongoing uncertainty.
This connection is currently being pursued.




\section{Proofs of results}
\subsection{Preliminaries for proofs}\label{app:gauss}

There are several generic results and inequalities that we use several times throughout our proofs, which we state here for completeness.
We write \(W_t = \left(W_t^{(1)}, \hdots, W_t^{(m)}\right)^{\T}\) as the components of the canonical \(m\)-dimensional Wiener process, where each \(W_t^{(i)}\) are mutually independent 1-dimensional Wiener processes.
The flow map \(F_0^t: \R^n \to \R^n\) summarises solutions of the deterministic model \eqref{eqn:ode_det}, given by
%as the unique solution to
%\begin{equation}
%	\dpd{F_{0}^{t}(x)}{t} = u\left(F_{0}^{t}(x), t\right), \quad F_{0}^{0}\left(x\right) = x,
%	\label{eqn:flow_map_ode}
%\end{equation}
%for \(x \in \R^n\), or equivalently
\begin{equation}
	F_{0}^{t}(x) = x + \int_{0}^{t}{u\left(F_{0}^{\tau}(x), \tau\right)\dif \tau},
	\label{eqn:flow_map_int}
\end{equation}
for an initial condition \(x \in \R^n\).
%By taking the gradient with respect to \(x\) of \eqref{eqn:flow_map_ode},
The spatial gradient (with respect to the initial condition) of the flow map solves the equation of variations associated with \eqref{eqn:ode_det}, i.e.
\begin{equation}
	\dpd{}{t}\nabla F_0^t(x) = \nabla u\left(F_0^t(x), t\right)\nabla F_0^t(x).
	\label{eqn:eqn_of_vars}
\end{equation}

For any real numbers \(x_1,\hdots,x_p \geq 0\) and \(r \geq 1\),
\begin{equation}
	\left(\sum_{i=1}^p{x_i}\right)^r \leq p^{r-1}\sum_{i=1}^p{x_i^r}.
	\label{eqn:trinomial}
\end{equation}
This results from an application of the finite form of Jensen's inequality.
An implication of the equivalence of the \(L^1\) and Euclidean norms and \eqref{eqn:trinomial} is that for any \(z \in \R^n\) and \(r \geq 1\),
\begin{equation}
	\norm{z}^r \leq \left(\sum_{i = 1}^n{\abs{z_i}}\right)^r \leq n^{r-1}\sum_{i=1}^n{\abs{z_i}^r},
	\label{eqn:norm_trinomial}
\end{equation}
where \(z_i\) denotes the \(i\)th component of \(z\).
If each component \(z_i\) of a vector \(z\) is bounded by a constant \(K\), then
\begin{equation}\label{eqn:bound_vector}
	\norm{z} \leq \sqrt{n} K.
\end{equation}
Similarly, if \(f: \R \to \R^n\) is a vector-valued function such that each component of \(f\) is integrable over an interval \([0,t]\), then for all \(r \geq 1\),
\begin{equation}
	\norm{\int_0^t{f\left(\tau\right)\dif\tau}}^r \leq t^{r-1}\int_0^t{\norm{f\left(\tau\right)}^r\dif\tau}.
	\label{eqn:convex_integral}
\end{equation}
This inequality results from an application of H\"{o}lder's inequality.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of \Cref{thm:main}}\label{app:main_thm_proof}
To prove the main result, we first require a lemma establishing a bound on the time integral of the expectation of the distance between the SDE solution and the reference deterministic trajectory.
% Results of this form are well-known\lb{For an SDE solution \(z_t\), a well-known bound is \(\avg{\sup_{\tau \in [0,t]}\norm{z_t}} \leq K\left(1 + \avg{\norm{z_0}}\right)\). Our result here is similar, but of course we are being more specific and explicitly stating the form of the bounds} for stochastic differential equation solutions, but here we make the distinction of writing the bounds explicitly.
% This result is an extension of Lemma 2.2 of \cite{Balasuriya_2020_StochasticSensitivityComputable} to arbitrary dimensions and accounting for an uncertain initial condition.

\begin{lemma}\label{lem:z_int_bound}
	Let \(q \geq 1\) be such that \(\delta_q < \infty\), then for all \(\epsilon > 0\) and \(\tau \in [0,T]\)
	\begin{equation*}
		\avg{\int_0^t{\norm{y_\tau^{(\epsilon)} - F_0^t\!\left(x_0\right)}^q\dif\tau}} \leq H_1(q,t)\epsilon^q + H_2(q,t)\delta_q^q,
	\end{equation*}
	where
	\begin{align*}
		H_1(q,t) & \coloneqq 3^{q-1} n^{3q/2} K_{\sigma}^{q/2} G_{q/2} t^{q/2 + 1}\exp\left(3^{q-1} K_{\nabla u}^q t^q\right) \\
		H_2(q,t) & \coloneqq 3^{q-1} t \exp\left(3^{q-1} K_{\nabla u}^q t^q\right),
	\end{align*}
	for a constant \(0 < S < \infty\).
\end{lemma}

\begin{proof}
	Consider the integral form of \eqref{eqn:sde_y},
	\[
		y_t^{(\epsilon)} = x + \int_0^t{u\left(y_\tau^{(\epsilon)}, \tau\right)\dif\tau} + \epsilon\int_0^t{\sigma\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}.
	\]
	Using \eqref{eqn:flow_map_int},
	\[
		y_t^{(\epsilon)} - F_0^t\!\left(x_0\right) = x - x_0 + \int_0^{t}{\left(u\left(y_\tau^{(\epsilon)}, \tau\right) - u\left(F_0^{\tau}\!\left(x_0\right), \tau\right)\right)\dif\tau} + \epsilon\int_0^t{\sigma\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau},
	\]
	and so
	\begin{equation}
		\avg{\norm{y_t^{(\epsilon)} - F_0^t\!\left(x_0\right)}^q} \leq \begin{multlined}[t]
			3^{q-1}\avg{\norm{x - x_0}^q} + 3^{q-1}t^{q-1}\avg{\int_0^{t}{\norm{u\!\left(y_\tau^{(\epsilon)}, \tau\right) - u\left(F_0^{\tau}\!\left(x_0\right), \tau\right)}^q\dif\tau}} \\
			+ 3^{q-1}\epsilon^q\avg{\norm{\int_0^t{\sigma\!\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}}^q},
		\end{multlined}
		\label{eqn:norm_y_t_tmp}
	\end{equation}
	using \eqref{eqn:trinomial} followed by \eqref{eqn:convex_integral}, and taking the expectation on both sides.

	Next, we establish a bound on the It\^o integral term in \eqref{eqn:norm_y_t_tmp}.
	For \(i \in \set{1,\hdots,n}\), let \(\sigma_{i\cdot}\) denote the \(i\)th row of \(\sigma\).
	Define the stochastic process
	\[
		M_\tau^{(i)} \coloneqq \sigma_{i\cdot}\!\left(y_\tau^{(\epsilon)}, \tau\right)
	\]
	for \(\tau \in [0,t]\), so that
	\[
		\left[\int_0^t{\sigma\!\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}\right]_i = \int_0^t{M_\tau^{(i)}\dif W_\tau}.
	\]
	Since \(y_t^{(\epsilon)}\) is a strong solution to \eqref{eqn:sde_y}, we have that (e.g. see Definition 6.1.1 of \cite{KallianpurSundar_2014_StochasticAnalysisDiffusion})
	\[
		\int_0^t{\norm{M_\tau^{(i)}}^2\dif\tau} \leq \int_0^t{nK_\sigma^2\dif\tau} < \infty, \quad \text{almost surely},
	\]
	so we can apply the Burkholder-Davis-Gundy inequality (e.g. \cite[Thm. 5.6.3]{KallianpurSundar_2014_StochasticAnalysisDiffusion}) to \(M_\tau\), which asserts that there exists a constant \(G_{q/2} > 0\) depending only on \(q\) such that
	\begin{align*}
		\avg{\abs{\int_0^t{M_\tau^{(i)}\dif W_\tau}}^{q}} & \leq G_{q/2} \avg{\left(\int_{0}^t{\norm{\sigma_{i\cdot}\!\left(y_\tau^{(\epsilon)}, \tau\right)}^2\dif\tau}\right)^{q/2}} \\
		                                                  & \leq G_{q/2} n^p K_{\sigma}^{q/2} t^{q/2},
	\end{align*}
	where the second inequality uses \ref{hyp:sigma_bounds}.
	Then,
	\begin{equation}
		\avg{\norm{\int_0^t{\sigma\!\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}}^{q}} \leq n^{3q/2} K_\sigma^{q/2} G_{q/2} t^{q/2},
		\label{eqn:z_sigma_bound}
	\end{equation}
	using \eqref{eqn:norm_trinomial}.

	Applying the bound \eqref{eqn:z_sigma_bound} to \eqref{eqn:norm_y_t_tmp}, we have
	\begin{equation}\label{eqn:y_F_diff_inter}
		\avg{\norm{y_t^{(\epsilon)}\!-\!F_0^t\!\left(x_0\right)}^q} \leq \begin{multlined}[t]
			3^{q-1}\delta_q^q
			+ 3^{q-1}t^{q-1}\avg{\int_0^{t}{\norm{u\!\left(y_\tau^{(\epsilon)}, \tau\right) - u\!\left(F_0^{\tau}\!\left(x_0\right), \tau\right)}}^q\dif\tau} \\
			+ 3^{q-1}\epsilon^q n^{3q/2}S^{q/2} G_{q/2} t^{q/2}.
		\end{multlined}
	\end{equation}
	We note that \(\avg{\norm{y_t^{(\epsilon)} - F_0^t\!\left(x\right)}^q} < \infty\) from \ref{hyp:u_bounds}, so by Tonelli's theorem (e.g. \cite[Thm. 2.3.9]{Bremaud_2020_ProbabilityTheoryStochastic}),
	\begin{equation*}
		\avg{\int_0^{t}{\norm{y_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right)}^q\dif\tau}} = \int_0^{t}{\avg{\norm{y_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right)}^q}\dif\tau}.
		\label{eqn:y_tonelli}
	\end{equation*}
	Now, using the Lipschitz continuity of \(\sigma\) from \ref{hyp:sigma_deriv_bound} on \eqref{eqn:y_F_diff_inter} and interchanging the expectation and integral,
	\[
		\avg{\norm{y_t^{(\epsilon)} - F_0^t\!\left(x_0\right)}^q} \leq \begin{multlined}[t]
			3^{q-1}\delta_q^q + 3^{q-1} K_{\nabla u}^q t^{q-1} \int_0^{t}{\avg{\norm{y_\tau^{(\epsilon)} - F_0^{\tau}\!\left(x_0\right)}^q}\dif\tau} \\
			+ 3^{q-1}\epsilon^q n^{3q/2}S^{q/2} G_{q/2} t^{q/2}.
		\end{multlined}
	\]
	Applying Gr\"{o}nwall's inequality and using the monotonicity of the resulting bound in \(t\), we have that for any \(\tau \in [0,t]\),
	\[
		\avg{\norm{y_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right)}^q}  \leq \begin{multlined}[t]
			3^{q-1}\epsilon^q n^{3q/2}S^{q/2} G_{q/2} t^{q/2}\exp\left(3^{q-1} K_{\nabla u}^q t^q\right) \\
			+ 3^{q-1}\exp\!\left(3^{q-1} K_{\nabla u}^q t^q\right)\delta_q^q
		\end{multlined}
	\]
	Integrating both sides with respect to time\lb{Possibly reconsider notation here - the jumps between \(\tau\) and \(t\) are confusing. Could reframe as a sup, but that might confound things further.} and again using Tonelli's theorem, we have
	\begin{align*}
		\avg{\int_0^t{\norm{y_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right)}^q\dif\tau}} \leq \begin{multlined}[t]
			                                                                                       3^{q-1}n^{3q/2}S^{q/2} G_{q/2} t^{q/2 + 1}\exp\!\left(3^{q-1} K_{\nabla u}^q t^q\right)\epsilon^q  \\
			                                                                                       + 3^{q-1}t\exp\left(3^{q-1} K_{\nabla u}^q t^q\right)\delta_q^q.
		                                                                                       \end{multlined}
	\end{align*}
	as desired.
\end{proof}

With these bounds established, we can now prove \Cref{thm:main}.
Subtracting the integral forms of \eqref{eqn:linear_sde_inform} and \eqref{eqn:sde_y} gives
\begin{align*}
	y_t^{(\epsilon)} - l_t^{(\epsilon)} & = \begin{multlined}[t]
		                                        \int_0^t{\left[u\!\left(y_\tau^{(\epsilon)}, \tau\right) - u\!\left(F_0^\tau\!\left(x_0\right), \tau\right) - \nabla u\!\left(F_0^\tau\!\left(x_0\right)\right)\left(l_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right)\right)\right]\dif\tau} \\
		                                        + \int_0^t{\left[\epsilon\sigma\!\left(y_\tau^{(\epsilon)}, \tau\right) - \epsilon\sigma\!\left(F_0^{\tau}\!\left(x_0\right), \tau\right)\right]\dif W_\tau}
	                                        \end{multlined}              \\
	                                    & = \begin{multlined}[t]
		                                        \int_0^t{\left[u\!\left(y_\tau^{(\epsilon)}, \tau\right) - \left(u\!\left(F_0^\tau\!\left(x_0\right), \tau\right) + \nabla u\!\left(F_0^\tau\!\left(x_0\right)\right)\left(y_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right)\right)\right)\right]\dif\tau} \\
		                                        + \int_0^t{\nabla u\!\left(F_0^\tau\!\left(x_0\right), \tau\right)\left[y_\tau^{(\epsilon)} - l_\tau^{(\epsilon)}\right]\dif \tau} \\
		                                        + \epsilon\int_0^t{ \left[\sigma\!\left(y_\tau^{(\epsilon)}, \tau\right) - \sigma\!\left(F_0^{\tau}\!\left(x_0\right), \tau\right)\right]\dif W_\tau}
	                                        \end{multlined} \\
	                                    & = A(t) + B(t) + \epsilon C(t),
\end{align*}
where
\begin{align*}
	A(t) & \coloneqq \int_0^t{\left[u\!\left(y_\tau^{(\epsilon)}, \tau\right) - \left(u\!\left(F_0^\tau\!\left(x_0\right), \tau\right) + \nabla u\!\left(F_0^\tau\!\left(x_0\right)\right)\left(y_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right)\right)\right)\right]\dif\tau} \\
	B(t) & \coloneqq \int_0^t{\nabla u\!\left(F_0^\tau\!\left(x_0\right), \tau\right)\left[y_\tau^{(\epsilon)} - l_\tau^{(\epsilon)}\right]\dif \tau}                                                                                                                          \\
	C(t) & \coloneqq \int_0^t{\left[\sigma\!\left(y_\tau^{(\epsilon)}, \tau\right) - \sigma\!\left(F_0^{\tau}\!\left(x_0\right), \tau\right)\right]\dif W_\tau}.
\end{align*}
Then, using \eqref{eqn:trinomial} and taking expectation,
\begin{equation}
	\avg{\norm{y_t^{(\epsilon)} - l_t^{(\epsilon)}}^r} \leq 3^{r-1}\left(\avg{\norm{A(t)}^r} + \avg{\norm{B(t)}^r} + \epsilon^r\avg{\norm{C(t)}^r}\right)
	\label{eqn:diff_decomp}
\end{equation}
First consider \(A(t)\), for which the integrand is the expected difference between the drift \(u\) evaluated along SDE solution and the first-order Taylor expansion of \(u\) about the reference deterministic trajectory.
Since for any \(t \in [0,T]\), \(u\left(\cdot, t\right)\) is twice continuously differentiable under \ref{hyp:coef_cont}, for each \(i = 1,\hdots,n\) there exists by Taylor's theorem (e.g. see \cite[Cor. A9.3.]{HubbardHubbard_2009_VectorCalculusLinear}) a function \(R_i: \R^n \times [0,T] \to \R\) such that
\begin{equation}
	u_i\!\left(z, \tau\right) = u_i\!\left(F_0^\tau\!\left(x_0\right), \tau\right) + \left[\nabla u_i\left(F_0^\tau\!\left(x_0\right), \tau\right)\right]\left(z - F_0^\tau\!\left(x_0\right)\right) + R_i\!\left(z, \tau\right)
	\label{eqn:taylor_expan}
\end{equation}
for any \(z \in \R^n\), where \(u_i\) denotes the \(i\)th component of \(u\).
The function \(R_i\) satisfies
\begin{equation}
	\abs{R_i(z, \tau)} \leq \frac12\norm{\nabla\nabla u_i\left(F_0^\tau(x), t\right)}\norm{z}^2 \leq \frac{K_{\nabla\nabla u}}{2}\norm{z - F_0^\tau\!\left(x_0\right)}^2.
	\label{eqn:rem_ineq}
\end{equation}
% Rearranging \eqref{eqn:taylor_expan},
% \[
% 	v_i^{(\epsilon)}\left(z, \tau\right) - \left[\nabla u_i\left(F_0^\tau(x), \tau\right)\right] z = \frac{1}{\epsilon}R_i(\epsilon z, \tau),
% \]
% where \(v_i^{(\epsilon)}\) is the \(i\)th component of \(v^{(\epsilon)}\).
Let \(R\!\left(z, \tau\right) \coloneqq \left( R_1\!\left(z, \tau\right), \hdots, R_n\!\left(z, \tau\right)\right)^{\T}\), then
\[
	A(t) = \int_0^t{R\left(y_{t}^{(\epsilon)}, \tau\right)\dif\tau},
\]
and since each component of \(R\) is bounded as in \eqref{eqn:rem_ineq}, using \eqref{eqn:bound_vector}
\[
	\norm{R\!\left(y_t^{(\epsilon)}, \tau\right)} \leq \frac{\sqrt{n} K_{\nabla\nabla u}}{2}\norm{y_t^{(\epsilon)} - F_0^\tau\!\left(x_0\right)}^2.
\]
Taking the norm and expectation then gives
\begin{align*}
	\avg{\norm{A(t)}^r} & = \avg{\norm{\int_0^t{R\!\left(y_t^{(\epsilon)}, \tau\right)\dif\tau}}^r}                                                                                                                  \\
	                    & \leq t^{r-1}\avg{\int_0^t{\norm{R\!\left(y_\tau^{(\epsilon)}, \tau\right)}^r\dif\tau}}                                                                                                     \\
	                    & \leq \frac{t^{r-1}n^{r/2}K_{\nabla\nabla u}^r}{2^r}\avg{\int_0^t{\norm{y_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right)}^{2r}\dif\tau}}                                                    \\
	                    & \leq \frac{t^{r-1} n^{r/2} K^r_{\nabla\nabla u}H_1(2r,t)}{2^r}\epsilon^{2r} + \frac{t^{r-1} n^{r/2} K_{\nabla\nabla u}^r H_2(2r, t)}{2^{r}}\delta_{2r}^{2r}, \numberthis\label{eqn:A_ineq}
\end{align*}
where the second inequality uses \eqref{eqn:convex_integral}, and \(H_1(2r,t)\) and \(H_2(2r,t)\) are obtained from \Cref{lem:z_int_bound} with \(q = 2r\).

Next, consider \(B(t)\), for which
\begin{equation}\label{eqn:B_ineq}
	\avg{\norm{B(t)}^r} \leq \int_0^t{t^{r-1}K_{\nabla u}^r\avg{\norm{y_\tau^{(\epsilon)} - l_t^{(\epsilon)}}^r}\dif\tau}.
\end{equation}
using \eqref{eqn:convex_integral} and then \ref{hyp:u_bounds}, and interchanging the expectation and the integral uses the fact that that \(\avg{\norm{y_\tau^{(\epsilon)}}} < \infty\) and \(\avg{\norm{l_\tau^{(\epsilon)}}} < \infty\).

Finally, consider \(C(t)\).
For each \(i \in \set{1,\hdots, n}\), define the stochastic process
\[
	N_\tau^{(i)} \coloneqq \sigma_{i\cdot}\left(y_\tau^{(\epsilon)},\tau\right) - \sigma_{i\cdot} \left(F_0^\tau\!\left(x_0\right), \tau\right).
\]
Then, the \(i\)th component of \(C(t)\) is
\[
	\left[C(t)\right]_i = \int_0^t{N_\tau^{(i)}\dif W_\tau}.
\]
From \ref{hyp:sigma_bounds} and using \eqref{eqn:bound_vector},
\[
	\int_0^t{\norm{N_\tau^{(i)}}^2\dif\tau} \leq \int_0^t{4nK_\sigma^2\dif\tau} < \infty,
\]
so we can apply the Burkholder-Davis-Gundy inequality on \(N_\tau^{(i)}\) to write
\begin{align*}
	\avg{\abs{\left[C(t)\right]_i}^{r}} & \leq G_{r/2}\avg{\left(\int_{0}^t{\norm{\sigma_{i\cdot}\left(y_\tau^{(\epsilon)}, \tau\right) - \sigma_{i\cdot} \left(F_0^\tau\!\left(x_0\right), \tau\right)}^2\dif\tau}\right)^{r/2}} \\
	                                    & \leq G_{r/2}\avg{\left(\int_0^{t}{ K_{\nabla\sigma}^2\norm{y_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right)}^2\dif\tau}\right)^{r/2}}                                                   \\
	                                    & \leq G_{r/2} K_{\nabla \sigma}^r t^{r/2 - 1}\avg{\int_0^t{\norm{y_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right)}^r\dif\tau}}                                                           \\
	                                    & \leq \begin{multlined}[t]
		                                           G_{r/2} K_{\nabla\sigma}^r t^{r/2 - 1} H_1(r, t)\epsilon^r + G_{r/2} K_{\nabla\sigma}^r t^{r/2 - 1} H_2\left(r,t\right)\delta_r^r,
	                                           \end{multlined} \numberthis\label{eqn:N_comp_ineq}
\end{align*}
where the second inequality uses the Lipschitz condition on \(\sigma\) in \eqref{eqn:sigma_lipschitz}, the third inequality uses \eqref{eqn:convex_integral}, and the fourth inequality uses \Cref{lem:z_int_bound} with \(q = r\).
Then, we have
\begin{equation}\label{eqn:C_ineq}
	\avg{\norm{C(t)}^r} \leq n^{r}G_{r/2} K_{\nabla\sigma}^r t^{r/2 - 1} H_1(r, t) \epsilon^r  + n^r G_{r/2} K_{\nabla\sigma}^r t^{r/2 - 1} H_2\left(r,t\right)\delta_r^r,
\end{equation}
using \eqref{eqn:norm_trinomial}, and then \eqref{eqn:N_comp_ineq}.


Combining \eqref{eqn:A_ineq}, \eqref{eqn:B_ineq} and \eqref{eqn:C_ineq} into \eqref{eqn:diff_decomp}, we have
\[
	\avg{\norm{y_t^{(\epsilon)} - l_t^{(\epsilon)}}^r} \leq \begin{multlined}[t]
		\frac{3^{r-1} t^{r-1}n^{r/2}K_{\nabla\nabla u}^r H_1(2r, t)}{2^r}\epsilon^{2r} + \frac{3^{r-1} t^{r-1} n^{r/2} K_{\nabla\nabla u}^r H_2(2r, t)}{2^r}\delta_{2r}^{2r} \\
		+ \int_0^t{3^{r-1}t^{r-1}K_{\nabla u}^r\avg{\norm{y_t^{(\epsilon)} - l_t^{(\epsilon)}}^r}\dif\tau} \\
		+ 3^{r-1}G_{r/2} K_{\nabla\sigma}^r t^{r/2 - 1} H_1(r, t)\epsilon^{2r} + 3^{r-1} G_{r/2} K_{\nabla\sigma}^r t^{r/2 - 1} H_2\left(r,t\right)\epsilon^r\delta_r^r.
	\end{multlined}
\]
Applying Gr\"{o}nwall's inequality, noting that \(H_1\) and \(H_2\) are non-decreasing in \(t\), we have
\[
	\avg{\norm{y_t^{(\epsilon)} - l_t^{(\epsilon)}}^r} \leq \begin{multlined}[t]
		\frac{3^{r-1} t^{r-1}n^{r/2}K_{\nabla\nabla u}^r H_1(2r, t)}{2^r}\exp\left(3^{r-1}t^r K_{\nabla u}^r\right)\epsilon^{2r} \\
		+ \frac{3^{r-1} t^{r-1} n^{r/2} K_{\nabla\nabla u}^r H_2(2r, t)}{2^r}\exp\left(3^{r-1}t^r K_{\nabla u}^r\right)\delta_{2r}^{2r} \\
		+ 3^{r-1}G_{r/2} K_{\nabla\sigma}^r t^{r/2 - 1}\exp\left(3^{r-1}t^r K_{\nabla u}^r\right) H_1(r, t)\epsilon^{2r} \\
		+ 3^{r-1} G_{r/2} K_{\nabla\sigma}^r t^{r/2 - 1}\exp\left(3^{r-1}t^r K_{\nabla u}^r\right) H_2\left(r,t\right)\epsilon^r\delta_r^r.
	\end{multlined}
\]
Set
\begin{subequations}\label{eqn:bound_defns}
	\begin{align}
		D_1(r,t) & \coloneqq 3^{r-1}\exp\left(3^{r-1} t^r K_{\nabla u}^r\right)\max\set{\frac{t^{r-1}n^{r/2}H_{1}(2r, t)}{2^r},\, G_{r/2} t^{r/2 - 1} H_1(r, t)} \\
		D_2(r,t) & \coloneqq 3^{r-1}t^{r-1} n^{r/2}H_{2}(2r, t)\exp\left(3^{r-1}t^r K_{\nabla u}^r\right)                                                        \\
		D_3(r,t) & \coloneqq 3^{r-1}G_{r/2} t^{r/2 - 1} H_2\left(r,t\right)\exp\left(3^{r-1}t^r K_{\nabla u}^r\right),
	\end{align}
\end{subequations}
then we have shown the desired result.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of \Cref{thm:limit_sol}}\label{app:limit_sol_proof}
We first establish that the It\^o integral of a matrix-valued deterministic function with respect to a multidimensional Wiener process is a multidimensional Gaussian process.
This is a well-known result in the scalar case, and the extension to our case is straightforward.
\begin{lemma}\label{lem:det_gauss}
	Let \(a,b \in \R\) and let \(g: [a,b] \to \R^{n\times n}\) be a matrix-valued deterministic function such that each element of \(g\) is It\^o-integrable.
	Consider the It\^o integral
	\[
		\mathcal{I}[g] \coloneqq \int_{a}^b{g(t)\dif W_t},
	\]
	Then, the integral \(\mathcal{I}[g]\) is a \(n\)-dimensional multivariate Gaussian random variable.
\end{lemma}
\begin{proof}
	For \(i,j \in \set{1,\hdots,n}\), let \(g_{ij}: [a,b] \to \R\) be the \((i,j)\)th element of \(g\).
	Then, let
	\[
		\mathcal{I}[g_{ij}] \coloneqq \int_a^b{g_{ij}(t)\dif W_t^{(i)}},
	\]
	so that the \(i\)th element of \(\mathcal{I}[g]\) is
	\[
		\mathcal{I}[g]_i = \sum_{j = 1}^n{\mathcal{I}\left[g_{ij}\right]}.
	\]
	Each \(\mathcal{I}[g_{ij}]\) is an It\^o integral of a deterministic, scalar-valued function with respect to a one-dimensional Brownian motion, which is well-known to be a Gaussian process (e.g. see \cite[Lem. 4.3.11]{Applebaum_2004_LevyProcessesStochastic}).
	Moreover, each element of \(\mathcal{I}[g]\) is the sum of independent Gaussian random variables and is therefore itself Gaussian.
	Hence, \(\mathcal{I}[g]\) follows a multivariate Gaussian distribution.
\end{proof}

Next, we show that the strong solution to the linearised SDE \eqref{eqn:linear_sde_inform} can be written as the independent sum \eqref{eqn:linear_sol}.
Let
\[
	M_t \coloneqq \frac{1}{\epsilon}\left[\nabla F_0^t\!\left(x_0\right)\right]^{-1}\left(l_t^{(\epsilon)} - F_0^t\!\left(x_0\right)\right),
\]
where \(l_t^{(\epsilon)}\) is the strong solution to \eqref{eqn:linear_sde_inform}.
Then, by It\^o's Lemma (e.g. see Theorem 5.5.1 of \cite{KallianpurSundar_2014_StochasticAnalysisDiffusion}),
\begin{align*}
	M_t & = \begin{multlined}[t]
		        \frac{1}{\epsilon}\left(x - x_0\right) + \frac{1}{\epsilon}\int_0^t\left(-\left[\nabla F_0^\tau\!\left(x_0\right)\right]^{-1} \dpd{\nabla F_0^\tau\!\left(x_0\right)}{\tau}\left[\nabla F_0^\tau\!\left(x_0\right)\right]^{-1}\left(l_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right) \right)\right. \\
		        \left. - \left[\nabla F_0^\tau\!\left(x_0\right)\right]^{-1}u\!\left(F_0^\tau\!\left(x_0\right), \tau\right) + \left[\nabla F_0^\tau\!\left(x_0\right)\right]^{-1}\nabla u\!\left(F_0^\tau\!\left(x_0\right), \tau\right) \left(l_\tau^{(\epsilon)} - F_0^\tau\!\left(x_0\right)\right) \right)\dif\tau \\
		        + \int_0^t{\left[\nabla F_0^\tau\!\left(x_0\right)\right]^{-1}\sigma\!\left(F_0^\tau\!\left(x_0\right), \tau\right)\dif W_\tau}
	        \end{multlined} \\
	    & = \frac{1}{\epsilon}\left(x - x_0\right) + \int_0^t{\left[\nabla F_0^\tau\!\left(x_0\right)\right]^{-1}\sigma\!\left(F_0^\tau\!\left(x_0\right), \tau\right)\dif W_\tau},
\end{align*}
where we reach the second line by using the fact that \(\nabla F_0^\tau\!\left(x_0\right)\) satisfies the equation of variations \eqref{eqn:eqn_of_vars}, and the fact that \(F_0^\tau\!\left(x_0\right)\) solves the deterministic ODE \eqref{eqn:ode_det}.
It follows that
\[
	l_t^{(\epsilon)} = \nabla F_0^t\!\left(x_0\right)\left(x - x_0\right) +  F_0^t\!\left(x_0\right) + \epsilon\int_0^t{\nabla F_0^t\!\left(x_0\right)\left[\nabla F_0^\tau\!\left(x_0\right)\right]^{-1} \sigma\!\left(F_0^\tau\!\left(x_0\right), \tau\right)\dif \tau}
\]
is a strong solution to \eqref{eqn:linear_sde_inform}, i.e. \eqref{eqn:linear_sol}.

By \eqref{hyp:init_indep}, \(\xi\) is independent of the Wiener process \(W_t\), and since independence is preserved under limits and linear transformations it follows that the It\^o integral \(\epsilon \nabla F_0^t\!\left(x_0\right)\int_0^t{L\!\left(x_0, \tau\right)\dif W_\tau}\) and \(\nabla F_0^t\!\left(x_0\right)\left(x - x_0\right)\) are independent.

Finally, we establish an explicit expression for the distribution of the It\^o integral term in \eqref{eqn:linear_sol}.
For any fixed \(t \in [0,T]\), the integrand is a deterministic, matrix-valued function, and is therefore follows a \(n\)-dimensional Gaussian distribution.
Moreover \cite{KallianpurSundar_2014_StochasticAnalysisDiffusion},
\[
	\avg{\int_0^t{L\!\left(x_0, \tau\right)\dif W_\tau}} = 0,
\]
and
\[
	\var{\int_0^t{L\!\left(x_0, \tau\right)\dif W_\tau}} = \avg{\left(\int_0^t{L\!\left(x_0, \tau\right)\dif W_\tau}\right)\left(\int_0^t{L\!\left(x_0, t, \tau\right)\dif W_\tau}\right)^{\T}}.
\]
Let \(L_{ij}\) denote the \((i,j)\)th element of \(L\), then the \((i,j)\)th element of the variance is
\begin{align*}
	\left[\var{\int_0^t{L\!\left(x_0, t, \tau\right)\dif\tau}}\right]_{ij} & = \avg{\left(\sum_{k=1}^m\int_0^t{L_{ik}\!\left(x_0, \tau\right)\dif W_{\tau}^{(k)}}\right)\left(\sum_{l=1}^m\int_0^t{L_{il}\!\left(x_0, \tau\right)\dif W_{\tau}^{(l)}}\right)}   \\
	                                                                       & = \sum_{k=1}^{m}\sum_{l=1}^m\avg{\left(\int_0^t{L_{ik}\!\left(x_0, \tau\right)\dif W_{\tau}^{(k)}}\right)\left(\int_0^t{L_{il}\!\left(x_0, \tau\right)\dif W_{\tau}^{(l)}}\right)} \\
	                                                                       & = \sum_{k=1}^m{\int_0^t{L_{ik}\!\left(x_0, \tau\right)L_{jk}\!\left(x_0, \tau\right)\dif\tau}},
\end{align*}
where the third equality uses It\^o's isometry \cite{KallianpurSundar_2014_StochasticAnalysisDiffusion} and the fact that \(W_t^{(k)}\) is independent of \(W_t^{(l)}\) for \(k \neq l\).
Hence, we have that
\[
	\int_0^t{L\!\left(x_0, \tau\right)\dif\tau} \isGauss{0, \int_0^t{L\!\left(x_0, \tau\right)L\!\left(x_0, \tau\right)^{\T}}\dif\tau},
\]
completing the proof of \Cref{thm:limit_sol}.




\subsection{Proof of \Cref{thm:limit_moments}}\label{app:limit_moments_proof}
It follows immediately from \eqref{eqn:linear_sol} that the mean of \(l_t^{(\epsilon)}\) is
\[
	\avg{l_t^{(\epsilon)}} = \avg{\nabla F_0^t\!\left(x_0\right) \left(x - x_0\right)} + F_0^t\!\left(x_0\right) + \epsilon \avg{\int_0^t{L\left(x,t,\tau\right)\dif\tau}} = \nabla F_0^t(x) \left(\avg{x} - x_0\right) + F_0^t\!\left(x_0\right),
\]
thus showing \eqref{eqn:mean_expl_eqn}.
% Differentiating the expression \eqref{eqn:mean_expl_eqn}, we get
% \begin{align*}
% 	\dod{\avg{l_t^{(\epsilon)}}}{t} & =  \dpd{\nabla F_0^t\!\left(x_0\right)}{t}\left(\avg{x} - x_0\right) + \dpd{F_0^t\!\left(x_0\right)}{t} + F_0^t\!\left(x_0\right) \\
% 	& = \nabla u\left(F_0^t\!\left(x_0\right), t\right) \nabla F_0^t(x) \left(\avg{x} - x_0\right) + u\!\left(F_0^t\!\left(x_0\right), t\right) \\
% 	& = \nabla u\left(F_0^t(x), t\right)\avg{l_t^{(\epsilon)}} + u\!\left(F_0^t\!\left(x_0\right)\right) - \nabla u\!\left(F_0^t\!\left(x_0\right), t\right) F_0^t\!\left(x_0\right).
% \end{align*}
% using the equation of variations \eqref{eqn:eqn_of_vars}.

Since the two summands in \eqref{eqn:linear_sol} are independent, the variance of \(l_t^{(\epsilon)}\) is
\begin{align*}
	\var{l_t^{(\epsilon)}} & = \var{\nabla F_0^t\!\left(x_0\right) \left(x - x_0\right)} + \var{\epsilon \nabla F_0^t\!\left(x_0\right)\int_0^t{L\!\left(x_0,\tau\right)\dif W_\tau}}                                      \\
	                       & = \nabla F_0^t\!\left(x_0\right) \left(\var{x} + \epsilon^2\int_0^t{L\!\left(x_0, \tau\right)L\!\left(x_0, \tau\right)^{\T} \dif\tau}\right) \left[\nabla F_0^t\!\left(x_0\right)\right]^{\T}
\end{align*}
where the variance of the It\^o integral was established in \Cref{app:limit_sol_proof}.

Finally, we show that \(\var{l_t^{(\epsilon)}}\) is the solution to the matrix differential equation \eqref{eqn:pi_ode}.
Differentiating the expression \eqref{eqn:pi_expl_eqn} % with the Leibniz integral rule
\begin{align*}
	\dod{\var{l_t^{(\epsilon)}}}{t} & = \begin{multlined}[t]
		                                    \dpd{\nabla F_0^t\!\left(x_0\right)}{t}\left(\var{x} + \epsilon^2 \int_0^t{L\!\left(x_0, \tau\right)L\!\left(x_0, \tau\right)^{\T}\dif\tau} \right)\left[\nabla F_0^t\!\left(x_0\right)\right]^{\T} \\
		                                    + \nabla F_0^t\!\left(x_0\right)\left(\var{x} + \epsilon^2 \int_0^t{L\!\left(x_0, \tau\right)L\!\left(x_0, \tau\right)^{\T}\dif\tau} \right)\left[\dpd{\nabla F_0^t\!\left(x_0\right)}{t}\right]^{\T} \\
		                                    + \epsilon^2\nabla F_0^t\!\left(x_0\right)L\!\left(x_0, t\right) L\!\left(x_0, t\right)\left[\nabla F_0^t\!\left(x_0\right)\right]^{\T}
	                                    \end{multlined}                                                                      \\
	                                & = \begin{multlined}[t]
		                                    \nabla u\!\left(F_0^t\!\left(x_0\right), t\right)\nabla F_0^t\!\left(x_0\right)\left(\var{x} + \epsilon^2 \int_0^t{L\!\left(x_0, \tau\right)L\!\left(x_0, \tau\right)^{\T}\dif\tau} \right)\left[\nabla F_0^t\!\left(x_0\right)\right]^{\T} \\
		                                    + \nabla F_0^t\!\left(x_0\right)\left(\var{x} + \epsilon^2 \int_0^t{L\!\left(x_0, \tau\right)L\!\left(x_0, \tau\right)^{\T}\dif\tau} \right)\left[\nabla F_0^t\!\left(x_0\right)\right]^{\T}\left[\nabla u\!\left(F_0^t\!\left(x_0\right), t\right)\right]^{\T} \\
		                                    + \epsilon^2\sigma\!\left(F_0^t\!\left(x_0\right), t\right)\sigma\!\left(F_0^t\!\left(x_0\right), t\right)^{\T}
	                                    \end{multlined}          \\
	                                & = \nabla u\!\left(F_0^t\!\left(x_0\right), t\right) \var{l_t^{(\epsilon)}} + \var{l_t^{(\epsilon)}}\left[\nabla u\!\left(F_0^t\!\left(x_0\right), t\right)\right]^{\T} + \epsilon^2\sigma\!\left(F_0^t\!\left(x_0\right), t\right)\sigma\!\left(F_0^t\!\left(x_0\right), t\right)^{\T},
\end{align*}
where the second inequality has used the equation of variations \eqref{eqn:eqn_of_vars}.


% While limits in distribution are not unique, we can define a new Gaussian random variable \(\zeta_t\left(x\right)\) with
%	\[
%		\zeta_t(x) \isGauss{0, \Sigma(x,t)},
%	\]
%	so that \(z_t^{(\epsilon)}\left(x\right)\) converges in distribution to \(\zeta_t\left(x\right)\) also.

% \section{Proofs for stochastic sensitivity}

% \subsection{Consistency with original work}\label{app:s2_consistency}
% We first assert that the generalised definition of stochastic sensitivity in \Cref{def:ss_Rn} is consistent with that presented in \(\R^2\) in the original work \cite{Balasuriya_2020_StochasticSensitivityComputable}.
% In \cite{Balasuriya_2020_StochasticSensitivityComputable}, stochastic sensitivity is defined as
% \[
% 	S^2\left(x,t\right) \coloneqq \lim_{\epsilon\downarrow 0}\sup_{\theta \in \left[-\pi/2, \pi/2\right)}\var{\begin{pmatrix}
% 		\cos\theta \\ \sin\theta
% 	\end{pmatrix}^{\T}z^{(\epsilon)}_t(x)}.
% \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of \Cref{thm:s2_calculation}}\label{app:s2_calculation_proof}
Let \(t \in [0,T]\) and consider the solutions \(y_t^{(\epsilon)}\) to \eqref{eqn:sde_y} and \(l_t^{(\epsilon)}\) to \eqref{eqn:linear_sde_inform} subject to the fixed initial condition \(x_0 \in \R^n\).
As this Theorem relates to the stochastic sensitivity of \cite{Balasuriya_2020_StochasticSensitivityComputable}, we follow the notation in that paper. Let
\[
	z_t^{(\epsilon)} = \frac{y_t^{(\epsilon)} - F_0^t\!\left(x_0\right)}{\epsilon},
\]
then stochastic sensitivity is given by
\begin{equation}
	S^2\!\left(x_0, t\right) = \lim_{\epsilon\downarrow 0}\sup\setc{\var{p^{\T} z_t^{(\epsilon)}}}{p \in \R^n, \norm{p} = 1}.
	\label{eqn:s2_z_def}
\end{equation}
\Cref{thm:main} then implies that for any \(r \geq 1\),
\begin{equation}
	\avg{\norm{z_t^{(\epsilon)} - \frac{l_t - F_0^t\!\left(x_0\right)}{\epsilon}}^r} \leq \left(K_{\nabla\nabla u}^r + K_{\nabla\sigma}^r\right) D_1(r,t)\epsilon^r.
	\label{eqn:main_ineq_z}
\end{equation}
We first establish that the \(\epsilon\)-limit and the variance operator in \eqref{eqn:s2_z_def} can be exchanged.
Using the fact that
\[
	\avg{\frac{l_t^{(\epsilon)} - F_0^t\!\left(x_0\right)}{\epsilon}} = 0
\]
and properties of expectation, we have
\begin{align*}
	\norm{\avg{z_t^{(\epsilon)}(x)}} & = \norm{\avg{z_t^{(\epsilon)}(x) - \frac{l_t^{(\epsilon)} - F_0^t\!\left(x_0\right)}{\epsilon}}}    \\
	                                 & \leq \avg{\norm{z_t^{(\epsilon)}(x) - \frac{l_t^{(\epsilon)} - F_0^t\!\left(x_0\right)}{\epsilon}}} \\
	                                 & \leq \left(K_{\nabla\nabla u} + K_{\nabla\sigma}\right) D_1(1, t)\epsilon,
\end{align*}
so
\[
	\lim_{\epsilon\downarrow 0}\norm{\avg{z_t^{(\epsilon)}(x)}} = 0.
\]
Next, we have that
\[
	\var{\frac{l_t^{(\epsilon} - F_0^t\!\left(x_0\right)}{\epsilon}} = \Sigma_0^t\!\left(x_0\right),
\]
and using the fact that \(\rho(z) = \norm{\avg{zz^{\T}}}^{1/2}\) defines a norm on the space of \(\R^n\)-valued random vectors and the reverse triangle inequality,
\begin{align*}
	\abs{\norm{\var{z_t^{(\epsilon)}(x)}}^{1/2} - \norm{\Sigma_0^t(x)}^{1/2}} & = \begin{multlined}[t]
		                                                                              \left|\norm{\avg{z_t^{(\epsilon)}(x)z_t^{(\epsilon)}(x)^{\T}} - \avg{z_t^{(\epsilon)}(x)}\avg{z_t^{(\epsilon)}(x)^{\T}}}^{1/2}\right. \\
		                                                                              - \left.\norm{\avg{\left(\frac{l_t^{(\epsilon)} - F_0^t\!\left(x_0\right)}{\epsilon}\right) \left(\frac{l_t^{(\epsilon} - F_0^t\!\left(x_0\right)}{\epsilon}\right)^{\T}}}^{1/2}\right|
	                                                                              \end{multlined} \\
	                                                                          & \leq \avg{\norm{z_t^{(\epsilon)}(x) - \frac{F_0^t\!\left(x_0\right)}{\epsilon}}^2}^{1/2} + \norm{\avg{z_t^{(\epsilon)}(x)}\avg{z_t^{(\epsilon)}(x)}^{\T}}^{1/2}                          \\
	                                                                          & \leq \left(K_{\nabla \nabla u}^2 + K_{\nabla\sigma}^2\right) D_1(2,t)\epsilon^2 + \norm{\avg{z_t^{(\epsilon)}(x)}},
\end{align*}
and so taking the limit as \(\epsilon\downarrow 0\) and squaring both sides, we have
\begin{equation}
	\lim_{\epsilon\downarrow 0}\norm{\var{z_t^{(\epsilon)}(x)}} = \norm{\Sigma_0^t(x)}.
	\label{eqn:sigma_lim}
\end{equation}
For \(\epsilon > 0\), define
\begin{align*}
	S^2_{(\epsilon)}(x,t) & \coloneqq \sup{\setc{\var{\frac{1}{\epsilon} p^{\T}\left(y_t^{(\epsilon)} - F_0^t\!\left(x_0\right)\right)}}{p \in \R^n, \, \norm{p} = 1}} \\
	                      & = \frac{1}{\epsilon^2}\sup{\setc{p^{\T}\var{y_t^{(\epsilon)}}p}{p \in \R^n, \, \norm{p} = 1}}
\end{align*}
Since \(\var{y_t^{(\epsilon)}}\) is symmetric and positive definite, the Cholesky decomposition provides an \(n \times n\) matrix \(\Pi^{(\epsilon)}\) such that \(\var{y_t^{(\epsilon)}} = \left[\Pi^{(\epsilon)}\right]^{\T}\Pi^{(\epsilon)}\), allowing us to write
\begin{align*}
	S^2_{(\epsilon)}(x,t) & = \sup\setc{\norm{\Pi^{(\epsilon)}p}^2}{p \in \R^n, \, \norm{p} = 1} \\
	                      & = \norm{\Pi^{(\epsilon)}}^2                                          \\
	                      & = \norm{\var{z_t^{(\epsilon)}(x)}}.
\end{align*}
using properties of the spectral norm.
Taking the limit as \(\epsilon\) approaches zero and using \eqref{eqn:sigma_lim},
\[
	S^2(x,t) = \lim_{\epsilon\downarrow 0} S^2_{(\epsilon)}(x,t) = \lim_{\epsilon\downarrow 0}\norm{\frac{1}{\epsilon^2}\var{l_t^{(\epsilon)}}} = \norm{\Sigma_0^t\!\left(x_0\right)}.
\]
Since \(\Sigma_0^t\!\left(x_0\right)\) is symmetric and positive definite, the operator norm, and therefore \(S^2\!\left(x_0,t\right)\), is given by the largest eigenvalue of \(\Sigma_0^t\!\left(x_0\right)\).


\section{Preliminaries for proofs}\label{app:gauss}

Throughout, we use the norm symbol \(\norm{\cdot}\) to denote (i) for a vector, the standard Euclidean vector norm, (ii) for a matrix, the spectral norm induced by the Euclidean norm, and (iii) for a 3rd-order tensor, the spectral norm induced by the matrix norm.
The gradient symbol \(\nabla\) generically refers to derivatives with respect to the state variable.
We write \(W_t = \left(W_t^{(1)}, \hdots, W_t^{(n)}\right)^{\T}\) as the components of the canonical \(n\)-dimensional Wiener process, where each \(W_t^{(i)}\) are mutually independent 1-dimensional Wiener processes.
The flow map \(F_0^t: \R^n \to \R^n\) summarises solutions of the deterministic model \eqref{eqn:ode_det}, given by
%as the unique solution to
%\begin{equation}
%	\dpd{F_{0}^{t}(x)}{t} = u\left(F_{0}^{t}(x), t\right), \quad F_{0}^{0}\left(x\right) = x,
%	\label{eqn:flow_map_ode}
%\end{equation}
%for \(x \in \R^n\), or equivalently
\begin{equation}
	F_{0}^{t}(x) = x + \int_{0}^{t}{u\left(F_{0}^{\tau}(x), \tau\right)\dif \tau}.
	\label{eqn:flow_map_int}
\end{equation}
for an initial condition \(x \in \R^n\).
%By taking the gradient with respect to \(x\) of \eqref{eqn:flow_map_ode},
The spatial gradient (with respect to the initial condition) of the flow map solves the equation of variations associated with \eqref{eqn:ode_det}, i.e.
\begin{equation}
	\dpd{}{t}\nabla F_0^t(x) = \nabla u\left(F_0^t(x), t\right)\nabla F_0^t(x).
	\label{eqn:eqn_of_vars}
\end{equation}
\Cref{hyp:smooth} summarises the minimum requirements of smoothness, measurability and boundedness in the coefficients of the main stochastic differential equation \eqref{eqn:sde_y}.

\renewcommand\thehypo{H}
\begin{hypo}\label{hyp:smooth}
	Let \(u: \R^n\times [0,T] \to \R^n\) and \(\sigma: \R^n \times [0,T] \to \R^{n\times n}\) be such that:
	\begin{enumerate}[label=(H.\arabic{*}), ref=H.\arabic{*}]
		\item\label{hyp:coef_cont} For each \(t \in [0,T]\), the function \(u(\cdot, t): \R^n \to \R^n\) given by \(u(x,t)\) is twice continuously differentiable on \(\R^n\), and each component of the function \(\sigma(\cdot, t): \R^n \to \R^{n\times n}\) given by \(\sigma(x,t)\) is differentiable on \(\R^n\).

		\item\label{hyp:coef_meas} For each \(x \in \R^n\), the function \(u(x,\cdot): [0,T] \to \R^n\) and each component of the function \(\sigma(x,\cdot): [0,T] \to \R^{n\times n}\) are Borel-measurable on \([0,T]\).

		\item\label{hyp:u_bounds} There exists a constant \(K_u > 0\) such that for any \(t \in [0,T]\) and \(x \in \R^n\),
		\begin{equation*}
			\max\set{\norm{u(x,t)},\, \norm{\nabla u(x,t)},\, \norm{\nabla \nabla u(x,t)}} \leq K_u.
		\end{equation*}

		% \item\label{hyp:F_exists} The flow map \(F_{t_1}^{t_2}: \R^n \to \R^n\) is well-defined and invertible for any \(t_1,t_2 \in [0,T]\).
		% Moreover, there exists a constant \(K_F > 0\) such that for any \(t_1, t_2 \in [0,T]\) and \(x \in \R^n\),
		% \[
		% 	\norm{\nabla F_{t_1}^{t_2}\left(x\right)} \leq K_F.
		% \]\lb{Don't think this is explicitly needed anywhere. Existence?}

		\item\label{hyp:sigma_bounds} There exists a constant \(K_\sigma > 0\) such that for any \(t \in [0,T]\) and \(x \in \R^n\),
		\begin{equation*}
			\max\set{\norm{\sigma(x,t)},\, \norm{\nabla{\sigma(x,t)}}} \leq K_{\sigma}.
		\end{equation*}
	\end{enumerate}

\end{hypo}
The conditions in \Cref{hyp:smooth} imply that \(u\) and \(\sigma\) are globally Lipschitz and guarantee that for each value of \(\epsilon\), there exists a unique strong solution to \eqref{eqn:sde_y} \cite{KallianpurSundar_2014_StochasticAnalysisDiffusion}.
Specifically, for every \(x,y \in \R^n\) and \(t \in [0,T]\),
\begin{equation}
	\norm{u\left(x,t\right) - u\left(y,t\right)} \leq K_u\norm{x - y},
	\label{eqn:u_lipschitz}
\end{equation}
and if \(\sigma_{i\cdot}\) denotes the \(i\)th row of \(\sigma\), then
\begin{equation}
	\norm{\sigma_{i\cdot}\left(x,t\right) - \sigma_{i\cdot}\left(y,t\right)} \leq K_\sigma \norm{x - y}.
	\label{eqn:sigma_lipschitz}
\end{equation}

The following inequalities are used several times throughout.
For any real numbers \(x_1,\hdots,x_m \geq 0\) and \(r \geq 1\),
\begin{equation}
	\left(\sum_{i=1}^m{x_i}\right)^r \leq m^{r-1}\sum_{i=1}^m{x_i^r}.
	\label{eqn:trinomial}
\end{equation}
This results from an application of the finite form of Jensen's inequality.
An implication of the equivalence of the \(L^1\) and Euclidean norms and \eqref{eqn:trinomial} is that for any \(z \in \R^n\) and \(r \geq 1\),
\begin{equation}
	\norm{z}^r \leq \left(\sum_{i = 1}^n{\abs{z_i}}\right)^r \leq n^{r-1}\sum_{i=1}^n{\abs{z_i}^r},
	\label{eqn:norm_trinomial}
\end{equation}
where \(z_i\) denotes the \(i\)th component of \(z\).
If each component \(z_i\) of a vector \(z\) is bounded by a constant \(K\), then
\begin{equation}\label{eqn:bound_vector}
	\norm{z} \leq \sqrt{n} K.
\end{equation}
Similarly, if \(f: \R \to \R^n\) is a vector-valued function such that each component of \(f\) is integrable over an interval \([0,t]\), then for all \(r \geq 1\),
\begin{equation}
	\norm{\int_0^t{f\left(\tau\right)\dif\tau}}^r \leq t^{r-1}\int_0^t{\norm{f\left(\tau\right)}^r\dif\tau}.
	\label{eqn:convex_integral}
\end{equation}
This inequality results from an application of H\"{o}lder's inequality.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of \Cref{thm:main}}\label{app:main_thm_proof}
% The first is a special case of the Burkh\"older-Davis-Gundy inequality.
% \begin{lemma}\label{lem:bdg}
% 	Suppose \(M = \set{M_\tau}\) is an It\^o-integrable stochastic process taking values in \(\R^n\) such that
% 	\[
% 		\int_0^t{\norm{M_\tau}^2\dif \tau} < \infty
% 	\]
% 	for all \(t \in [0,T]\).
% 	Then, for any \(t \in [0,T]\) and \(p > 0\), there is a constant \(C_p\) depending only on \(p\) such that
% 	\[
% 		\avg{\abs{\int_0^t{M_\tau\dif W_\tau}}^{2p}} \leq C_p\avg{\left(\int_0^t{\norm{M_\tau}^2\dif\tau}\right)^p}.
% 	\]
% \end{lemma}
% \begin{proof}
% 	Since \(M_\tau\) is It\^o integrable, the stochastic process \(\set{\int_0^t{M_\tau\dif W_\tau}}\) is a continuous and appropriately filtration-adapted local martingale.
% 	Then, by the standard form of the Burkh\"{o}lder-Davis-Gundy inequality (e.g. see \cite[Thm. 5.6.3]{KallianpurSundar_2014_StochasticAnalysisDiffusion}),
% 	\[
% 		\avg{\abs{\int_0^t{M_\tau\dif W_\tau}}^{2p}} \leq \avg{\sup_{s \in [0,t]}\abs{\int_0^s{M_\tau\dif W_\tau}}^{2p}} \leq C_p\avg{\left(\int_0^t{\norm{M_\tau}^2\dif\tau}\right)^p}.
% 	\]
% \end{proof}

To prove the main result, we first require several lemmas.
The first lemma is an extension of \cite[Lem. A.1]{Balasuriya_2020_StochasticSensitivityComputable} to the \(n\)-dimensional case.
\begin{lemma}\label{lem:sigma_specific}
	Let \(y_t^{(\epsilon)}\) denote the strong solution to \eqref{eqn:sde_y} for \(t \in [0,T]\), then for any \(p > 0\) there exists a constant \(G_p > 0\) such that
	\[
		\avg{\norm{\int_0^{t}{\sigma\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}}^{2p}} \leq n^{3p} K_\sigma^{2p}G_p t^p.
	\]

	% there exists a constant \(H_{\sigma, p}\) independent of \(\epsilon\) and \(t\) such thatappen
	% \[
	% 	\avg{\norm{\int_0^{t}{\sigma\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}}^{2p}} \leq H_{\sigma, p}.
	% \]
\end{lemma}
\begin{proof}
	For \(i \in \set{1,\hdots,n}\), let \(\sigma_{i\cdot}\) denote the \(i\)th row of \(\sigma\).
	Define the stochastic process
	\[
		M_\tau^{(i)} \coloneqq \sigma_{i\cdot}\left(y_\tau^{(\epsilon)}, \tau\right)
	\]
	for \(\tau \in [0,t]\), so that
	\[
		\left[\int_0^t{\sigma\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}\right]_i = \int_0^t{M_\tau^{(i)}\dif W_\tau}.
	\]
	% Since \(y_\tau^{(\epsilon)}\) is \(\mathcal{F}\)-measurable and \(\mathcal{F}_\tau\)-adapted, by virtue of being a strong solution to \eqref{eqn:sde_y}, and \(\sigma\) is continuous with respect to the spatial coordinate, we have that \(M_\tau^{(i)}\) is also \(\mathcal{F}\)-measurable and \(\mathcal{F}_\tau\)-adapted
	Then, from \ref{hyp:sigma_bounds} and using \eqref{eqn:bound_vector},
	\[
		\int_0^t{\norm{M_\tau^{(i)}}^2\dif\tau} \leq \int_0^t{nK_\sigma^2\dif\tau} < \infty,
	\]
	so we can apply the Burkholder-Davis-Gundy inequality (e.g. \cite[Thm. 5.6.3]{KallianpurSundar_2014_StochasticAnalysisDiffusion}) to \(M_\tau\), which asserts that there exists a constant \(G_p > 0\) depending only on \(p\) such that
	\begin{align*}
		\avg{\abs{\int_0^t{M_\tau^{(i)}\dif W_\tau}}^{2p}} & \leq G_{p} \avg{\left(\int_{0}^t{\norm{\sigma_{i\cdot}\left(y_\tau^{(\epsilon)}, \tau\right)}^2\dif\tau}\right)^p} \\
		% & \leq G_{p} \avg{\left(\int_{0}^t{nK_\sigma^2\dif\tau}\right)^p}                                                    \\
		                                                   & \leq G_{p} n^p K_\sigma^{2p} t^p. \numberthis\label{eqn:M_ij_bound}
	\end{align*}
	where the second inequality uses \ref{hyp:sigma_bounds} and \eqref{eqn:bound_vector}.
	Then, we can write
	\begin{align*}
		\avg{\norm{\int_0^t{\sigma\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}}^{2p}} % & \leq \avg{n^{2p-1}\sum_{i=1}^{n}{\abs{\int_0^t{M_\tau^{(i)}\dif W_\tau}}^{2p}}} \\
		% & \leq n^{2p-1}\sum_{i=1}^{n}{\avg{\abs{\int_0^t{\sigma_{i\cdot}\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}}^{2p}}} \\
		 & \leq n^{3p} K_\sigma^{2p} G_p t^p,
	\end{align*}
	using \eqref{eqn:norm_trinomial} and then \eqref{eqn:M_ij_bound}.
\end{proof}

The next Lemma is a similar extension of \cite[Lem. 2.2]{Balasuriya_2020_StochasticSensitivityComputable}.
\begin{lemma}\label{lem:z_int_bound}
	Let \(q \geq 1\), then for all \(\epsilon > 0\) and \(\tau \in [0,T]\)
	\begin{equation*}
		\avg{\int_0^t{\norm{z_\tau^{(\epsilon)}(x)}^q\dif\tau}} \leq H_q(t),
	\end{equation*}
	where
	\[
		H_q(t) \coloneqq 2^{q-1} n^{3q/2}K_\sigma^q G_{q/2} t^{q/2 + 1}\exp\left(2^{q-1} K_u^q t^q\right).
	\]
\end{lemma}
\begin{proof}
	Consider the integral form of \eqref{eqn:sde_y},
	\[
		y_t^{(\epsilon)} = x + \int_0^t{u\left(y_\tau^{(\epsilon)}, \tau\right)\dif\tau} + \epsilon\int_0^t{\sigma\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}.
	\]
	Using \eqref{eqn:flow_map_int},
	\[
		y_t^{(\epsilon)} - F_0^t(x) = \int_0^{t}{\left(u\left(y_\tau^{(\epsilon)}, \tau\right) - u\left(F_0^{\tau}(x), \tau\right)\right)\dif\tau} + \epsilon\int_0^t{\sigma\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau},
	\]
	and so
	\begin{align*}
		\norm{y_t^{(\epsilon)} - F_0^t(x)}^q % & \leq 2^{q-1}\!\left(\norm{\int_0^{t}{\left(u\left(y_\tau^{(\epsilon)}, \tau\right) - u\left(F_0^{\tau}(x), \tau\right)\right)\dif\tau}}^q + \epsilon^q\norm{\int_0^t{\sigma\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}}^q \right) \\
		 & \leq 2^{q-1}\!\left(t^{q-1}\int_0^{t}{\norm{u\left(y_\tau^{(\epsilon)}, \tau\right) \!-\! u\left(F_0^{\tau}(x), \tau\right)}^q\dif\tau} + \epsilon^q\norm{\int_0^t{\sigma\left(y_\tau^{(\epsilon)}, \tau\right)\dif W_\tau}}^q \right),
	\end{align*}
	using \eqref{eqn:trinomial} and then \eqref{eqn:convex_integral}.
	Taking the expectation and applying \Cref{lem:sigma_specific} with \(p = q/2\) gives
	\begin{align*}
		\avg{\norm{y_t^{(\epsilon)}\!-\!F_0^t(x)}^q} & \leq 2^{q-1}t^{q-1}\avg{\int_0^{t}{\norm{u\left(y_\tau^{(\epsilon)}, \tau\right) - u\left(F_0^{\tau}(x), \tau\right)}}^q\dif\tau} \\
		                                             & \qquad\qquad\qquad\qquad + 2^{q-1}\epsilon^q n^{3q/2}K_\sigma^q G_{q/2} t^{q/2}. \numberthis\label{eqn:y_F_diff_inter}
	\end{align*}
	We note that \(\avg{\norm{y_t^{(\epsilon)} - F_0^t(x)}^q} < \infty\) from \ref{hyp:u_bounds}, so by Tonelli's theorem (e.g. \cite[Thm. 2.3.9]{Bremaud_2020_ProbabilityTheoryStochastic}),
	% \begin{align*}
	% 	\avg{\norm{y_t^{(\epsilon)} - F_0^t(x)}^q} & \leq 2^{q-1}t^{q-1}\avg{\int_0^{t}{\left(\norm{u\left(y_\tau^{(\epsilon)}, \tau\right)} + \norm{u\left(F_0^{\tau}(x), \tau\right)}\right)^q\dif\tau}} \\
	%  & \qquad\qquad\qquad\qquad + 2^{q-1}\epsilon^q n^{3q/2}K_\sigma^q G_{q/2} t^{q/2} \\
	% 	                                           & \leq 2^{q-1}t^{q-1}\avg{\int_0^{t}{2^qK_u^q\dif\tau}} + 2^{q-1}\epsilon^q n^{3q/2}K_\sigma^q G_{q/2} t^{q/2} \\
	% 	% & \leq 2^{2q-1}T^{q}K_u^q + 2^{q-1}\epsilon^q n^{3q/2}K_\sigma^q G_{q/2}^T{q/2} \\
	% 	                                           & < \infty,
	% \end{align*}
	% using \eqref{hyp:u_bounds}.
	\begin{equation*}
		\avg{\int_0^{t}{\norm{y_\tau^{(\epsilon)} - F_0^\tau(x)}^q\dif\tau}} = \int_0^{t}{\avg{\norm{y_\tau^{(\epsilon)} - F_0^\tau(x)}^q}\dif\tau}.
		\label{eqn:y_tonelli}
	\end{equation*}
	Now, using the Lipschitz condition \eqref{eqn:u_lipschitz} on \eqref{eqn:y_F_diff_inter} and interchanging the expectation and integral,
	\begin{align*}
		\avg{\norm{y_t^{(\epsilon)} - F_0^t(x)}^q} % & \leq 2^{q-1}K_u^qt^{q-1}\avg{\int_0^{t}{\norm{y_\tau^{(\epsilon)} - F_0^{\tau}(x)}^q\dif\tau}} + 2^{q-1}\epsilon^q n^{3q/2}K_\sigma^q G_{q/2} t^{q/2}  \\
		 & \leq 2^{q-1} K_u^q t^{q-1} \int_0^{t}{\avg{\norm{y_\tau^{(\epsilon)} - F_0^{\tau}(x)}^q}\dif\tau} + 2^{q-1}\epsilon^q n^{3q/2}K_\sigma^q G_{q/2} t^{q/2}.
	\end{align*}
	Applying Gr\"{o}nwall's inequality then gives
	\begin{align*}
		\avg{\norm{y_t^{(\epsilon)} - F_0^t(x)}^q} % & \leq 2^{q-1}\epsilon^q n^{3q/2}K_\sigma^q G_{q/2} t^{q/2} \exp\left(\int_{0}^t{2^{q-1} K_u^q t^{q-1}\dif\tau}\right) \\
		 & \leq 2^{q-1}\epsilon^q n^{3q/2}K_\sigma^q G_{q/2} t^{q/2}\exp\left(2^{q-1} K_u^q t^q\right).
	\end{align*}
	Hence, for \(\tau \leq t\) we have
	\begin{equation*}
		\avg{\norm{z_\tau^{(\epsilon)}(x)}^q} \leq 2^{q-1} n^{3q/2}K_\sigma^q G_{q/2} \tau^{q/2}\exp\left(2^{q-1} K_u^q \tau^q\right)< \infty,
		\label{eqn:z_exp_finite}
	\end{equation*}
	and so Tonelli's theorem allows us to again interchange the expectation and integral so that
	\begin{align*}
		\avg{\int_0^t{\norm{z_{\tau}^{(\epsilon)}(x)}^q\dif\tau}} & = \int_0^t{\avg{\norm{z_\tau^{(\epsilon)}(x)}^q}\dif\tau}                              \\
		% & \leq \int_0^t{2^{q-1} n^{3q/2}K_\sigma^q G_{q/2} \tau^{q/2}\exp\left(2^{q-1} K_u^q \tau^q\right)\dif\tau} \\
		                                                          & \leq 2^{q-1} n^{3q/2}K_\sigma^q G_{q/2} t^{q/2 + 1}\exp\left(2^{q-1} K_u^q t^q\right),
	\end{align*}
	as desired.
\end{proof}


We now have all the tools to prove \Cref{thm:main}.
% Since \(z_t(x)\) is a strong solution of \eqref{eqn:limit_sde} by \Cref{lem:z_lim_sol}, we can write
% \begin{equation}
% 	z_t(x) = \int_0^t{\nabla u\left(F_0^\tau(x), \tau\right) z_\tau(x)\dif \tau} + \int_0^t{\sigma \left(F_0^\tau(x), \tau\right)\dif W_\tau}.
% 	\label{eqn:z_lim_int}
% \end{equation}
By rearranging the integral forms of \eqref{eqn:sde_y} and using \eqref{eqn:flow_map_int} and \eqref{eqn:z_def}, \(z_t^{(\epsilon)}(x)\) satisfies the integral equation
\begin{equation}
	z_t^{(\epsilon)}(x) = \int_0^t{v^{(\epsilon)}\left(z_\tau^{(\epsilon)}(x), \tau\right)\dif\tau} + \int_0^t{\sigma\left(\epsilon z_\tau^{(\epsilon)}(x) + F_0^{\tau}(x),\tau\right) \dif W_\tau},
	\label{eqn:z_eps_int}
\end{equation}
where
\[
	v^{(\epsilon)}\left(z_\tau^{(\epsilon)}, \tau\right) \coloneqq \frac1\epsilon\left[u\left(\epsilon z_\tau^{(\epsilon)} + F_0^\tau(x), \tau\right) - u\left(F_0^\tau(x), \tau\right)\right].
\]
Subtracting the integral form of \eqref{eqn:limit_sde} from \eqref{eqn:z_eps_int} gives
\begin{align*}
	z_t^{(\epsilon)}(x) - z_t(x) & = \int_0^t{\left[v^{(\epsilon)}\left(z_\tau^{(\epsilon)}(x), \tau\right) - \nabla u\left(F_0^\tau(x), \tau\right) z_\tau(x)\right]\dif\tau}                                \\
	                             & \qquad\qquad\qquad\qquad + \int_0^t{\left[\sigma\left(\epsilon z_\tau^{(\epsilon)} + F_0^{\tau}(x),\tau\right) - \sigma \left(F_0^\tau(x), \tau\right)\right] \dif W_\tau} \\
	% & = \int_0^t{\left[v^{(\epsilon)}\left(z_\tau^{(\epsilon)}(x), \tau\right) - \nabla u\left(F_0^\tau(x), \tau\right) z_\tau^{(\epsilon)}(x)\right]\dif\tau} + \int_0^t{\nabla u\left(F_0^\tau(x), \tau\right)\left[z_\tau^{(\epsilon)}(x) - z_\tau(x)\right]\dif\tau}                           \\
	% & \qquad\qquad + \int_0^t{\left[\sigma\left(\epsilon z_\tau^{(\epsilon)}(x) + F_0^{\tau}(x),\tau\right) - \sigma \left(F_0^\tau(x), \tau\right)\right] \dif W_\tau}                                                                                                                            \\
	                             & = A(t) + B(t) + C(t),
\end{align*}
where
% \begin{subequations}
\begin{align*}
	A(t) & \coloneqq \int_0^t{\left[v^{(\epsilon)}\left(z_\tau^{(\epsilon)}(x), \tau\right) - \nabla u\left(F_0^\tau(x), \tau\right) z_\tau^{(\epsilon)}(x)\right]\dif\tau} \\
	B(t) & \coloneqq \int_0^t{\nabla u\left(F_0^\tau(x), \tau\right)\left[z_\tau^{(\epsilon)}(x) - z_\tau(x)\right]\dif\tau}                                                \\
	C(t) & \coloneqq \int_0^t{\left[\sigma\left(\epsilon z_\tau^{(\epsilon)}(x) + F_0^{\tau}(x),\tau\right) - \sigma \left(F_0^\tau(x), \tau\right)\right] \dif W_\tau}.
\end{align*}
Then, using \eqref{eqn:trinomial} and taking expectation,
% \[
% 	\norm{z_t^{(\epsilon)}(x) - z_t(x)}^r %\leq \left(\norm{A_\epsilon(t)} + \norm{B_\epsilon(t)} + \norm{C(t)}\right)^r
%  \leq 3^{r-1}\left(\norm{A(t)}^r + \norm{B(t)}^r + \norm{C(t)}^r\right)
% \]
% and so
\begin{equation}
	\avg{\norm{z_t^{(\epsilon)}(x) - z_t(x)}^r} \leq 3^{r-1}\left(\avg{\norm{A(t)}^r} + \avg{\norm{B(t)}^r} + \avg{\norm{C(t)}^r}\right)
	\label{eqn:diff_decomp}
\end{equation}
First consider \(A(t)\).
Since for any \(t \in [0,T]\), \(u\left(\cdot, t\right)\) is twice continuously differentiable under \ref{hyp:coef_cont}, for each \(i = 1,\hdots,n\) there exists by Taylor's theorem (e.g. see \cite[Cor. A9.3.]{HubbardHubbard_2009_VectorCalculusLinear}) a function \(R_i: \R^n \times [0,T] \to \R\) such that
\begin{equation}
	u_i\left(\epsilon z + F_0^\tau(x), \tau\right) = u_i\left(F_0^\tau(x), \tau\right) + \epsilon\left[\nabla u_i\left(F_0^\tau(x), \tau\right)\right]z + R_i(\epsilon z, \tau)
	\label{eqn:taylor_expan}
\end{equation}
for any \(z \in \R^n\), where \(u_i\) denotes the \(i\)th component of \(u\).
The function \(R_i\) satisfies
\begin{equation}
	\abs{R_i(\epsilon z, \tau)} \leq \frac12\norm{\nabla\nabla u_i\left(F_0^\tau(x), t\right)}\norm{\epsilon z}^2 \leq \frac{\epsilon^2 K_u}{2}\norm{z}^2.
	\label{eqn:rem_ineq}
\end{equation}
Rearranging \eqref{eqn:taylor_expan},
\[
	v_i^{(\epsilon)}\left(z, \tau\right) - \left[\nabla u_i\left(F_0^\tau(x), \tau\right)\right] z = \frac{1}{\epsilon}R_i(\epsilon z, \tau),
\]
where \(v_i^{(\epsilon)}\) is the \(i\)th component of \(v^{(\epsilon)}\).
Let \(R\left(\epsilon z, \tau\right) \coloneqq \left( R_1\left(\epsilon z, \tau\right), \hdots, R_n\left(\epsilon z, \tau\right)\right)^{\T}\), then
\[
	A(t) = \frac{1}{\epsilon} \int_0^t{R\left(\epsilon z_\tau^{(\epsilon)}(x), \tau\right)\dif\tau},
\]
and since each component of \(R\) is bounded as in \eqref{eqn:rem_ineq}, using \eqref{eqn:bound_vector}
\[
	\norm{R\left(\epsilon z, \tau\right)} \leq \frac{\sqrt{n} \epsilon^2 K_u}{2}\norm{z}^2.
\]
Taking the norm and expectation then gives
\begin{align*}
	\avg{\norm{A(t)}^r} & = \frac{1}{\epsilon^r}\avg{\norm{\int_0^t{R\left(\epsilon z_\tau^{(\epsilon)}(x), \tau\right)\dif\tau}}^r}            \\
	                    & \leq \frac{t^{r-1}}{\epsilon^r}\avg{\int_0^t{\norm{R\left(\epsilon z^{(\epsilon)}_\tau(x), \tau\right)}^r\dif\tau}}   \\
	                    & \leq \frac{t^{r-1}n^{r/2}\epsilon^{2r}K_u^r}{2^r\epsilon^r}\avg{\int_0^t{\norm{z_\tau^{(\epsilon)}(x)}^{2r}\dif\tau}} \\
	                    & \leq \frac{t^{r-1}n^{r/2}\epsilon^{r}K_u^rH_{2r}(t)}{2^r}, \numberthis\label{eqn:A_ineq}
\end{align*}
where the second inequality uses \eqref{eqn:convex_integral}, and \(H_{2r}(t)\) is obtained from \Cref{lem:z_int_bound} with \(q = 2r\).

Next, consider \(B(t)\), for which
\begin{align*}
	\avg{\norm{B(t)}^r} %& = \avg{\norm{\int_0^t{\nabla u\left(F_0^\tau(x), \tau\right)\left[z_\tau^{(\epsilon)}(x) - z_\tau(x)\right]\dif\tau}}^r}           \\
	% & \leq t^{r-1}\avg{\int_0^t{\norm{\nabla u\left(F_0^\tau(x), \tau\right)\left[z_\tau^{(\epsilon)}(x) - z_\tau(x)\right]}^r\dif\tau}} \\
	% & \leq t^{r-1}K_u^r\avg{\int_0^t{\norm{z_\tau^{(\epsilon)}(x) - z_\tau(x)}^r\dif\tau}} \\
	 & \leq \int_0^t{t^{r-1}K_u^r\avg{\norm{z_\tau^{(\epsilon)}(x) - z_\tau(x)}^r}\dif\tau}.\numberthis\label{eqn:B_ineq}
\end{align*}
using \eqref{eqn:convex_integral} and then \ref{hyp:u_bounds}, and interchanging the expectation and the integral uses the fact that that \(\avg{\norm{z_\tau^{(\epsilon)}(x)}} < \infty\) and \(\avg{\norm{z_\tau(x)}} < \infty\).
% Now, for any \(\tau \in [0,T]\),
% \begin{align*}
% 	\avg{\norm{z_\tau^{(\epsilon)}(x) - z_\tau(x)}^r} & \leq \avg{\left(\norm{z_\tau^{(\epsilon)}(x)} + \norm{z_\tau(x)}\right)^r}                 \\
% 	                                                  & \leq 2^{r-1}\avg{\norm{z_\tau^{(\epsilon)}(x)}^r + \norm{z_\tau(x)}^r}                     \\
% 	                                                  & \leq 2^{r-1}\left(\avg{\norm{z_\tau^{(\epsilon)}(x)}^r} + \avg{\norm{z_\tau(x)}^r}\right).
% \end{align*}
% where the second inequality follows from \eqref{eqn:trinomial}.
% Now, from \eqref{eqn:z_exp_finite} we have \(\avg{\norm{z_\tau^{(\epsilon)}(x)}^r} < \infty\) and similarly we have \(\avg{\norm{z_\tau(x)}^r} < \infty\).
% So we have,
% \begin{align*}
% 	\avg{\norm{z_\tau^{(\epsilon)}(x) - z_\tau(x)}^r} < \infty,
% \end{align*}
% which allows the interchange of the expectation and the integral by Tonelli's theorem in \eqref{eqn:B_temp}, to write

Finally, consider \(C(t)\).
For each \(i \in \set{1,\hdots, n}\), define the stochastic process
\[
	N_t^{(i)} \coloneqq \sigma_{i\cdot}\left(\epsilon z_\tau^{(\epsilon)}(x) + F_0^{\tau}(x),\tau\right) - \sigma_{i\cdot} \left(F_0^\tau(x), \tau\right).
\]
Then, the \(i\)th component of \(C(t)\) is
\[
	\left[C(t)\right]_i = \int_0^t{N_\tau^{(i)}\dif W_\tau}.
\]
From \ref{hyp:sigma_bounds} and using \eqref{eqn:bound_vector},
\[
	\int_0^t{\norm{N_\tau^{(i)}}^2\dif\tau} \leq \int_0^t{4nK_\sigma^2\dif\tau} < \infty,
\]
so we can apply the Burkholder-Davis-Gundy inequality on \(N_\tau^{(i)}\) to write
\begin{align*}
	\avg{\abs{\left[C(t)\right]_i}^{r}} & \leq G_{r/2}\avg{\left(\int_{0}^t{\norm{\sigma_{i\cdot}\left(\epsilon z_\tau^{(\epsilon)}(x) + F_0^{\tau}(x),\tau\right) - \sigma_{i\cdot} \left(F_0^\tau(x), \tau\right)}^2\dif\tau}\right)^{r/2}} \\
	                                    & \leq G_{r/2}\avg{\left(\int_0^{t}{ K_\sigma^2\norm{\epsilon z_\tau^{(\epsilon)}(x)}^2\dif\tau}\right)^{r/2}}                                                                                        \\
	                                    & \leq G_{r/2} K_\sigma^r \epsilon^r t^{r/2 - 1}\avg{\int_0^t{\norm{z_\tau^{(\epsilon)}(x)}^r\dif\tau}}                                                                                               \\
	                                    & \leq G_{r/2} K_\sigma^r \epsilon^r t^{r/2 - 1}H_{r}(t), \numberthis\label{eqn:N_comp_ineq}
\end{align*}
where the second inequality uses the Lipschitz condition on \(\sigma\) in \eqref{eqn:sigma_lipschitz}, the third inequality uses \eqref{eqn:convex_integral}, and the fourth inequality uses \Cref{lem:z_int_bound} with \(q = r\).
Then, we have
\begin{align*}
	\avg{\norm{C(t)}^r} % & \leq \avg{n^{r - 1}\sum_{i=1}^n{\abs{\left[C(t)\right]_i}^r}}               \\
	% & \leq n^{r - 1}\sum_{i=1}^{n}{\avg{\abs{\left[C_\epsilon(t)\right]_i}^r}}             \\
	 & \leq n^{r}G_{r/2}K_u^r \epsilon^r t^{r/2 - 1}H_{r}(t), \numberthis\label{eqn:C_ineq}
\end{align*}
using \eqref{eqn:norm_trinomial}, and then \eqref{eqn:N_comp_ineq}.


Combining \eqref{eqn:A_ineq}, \eqref{eqn:B_ineq} and \eqref{eqn:C_ineq} into \eqref{eqn:diff_decomp}, we have
\begin{align*}
	\avg{\norm{z_t^{(\epsilon)}(x) - z_t(x)}^r} % & \leq 3^{r-1}\left(\avg{\norm{A_\epsilon(t)}^r} + \avg{\norm{B_\epsilon(t)}^r} + \avg{\norm{C_\epsilon(t)}^r}\right)              \\
	 & \leq 3^{r-1}\left(\frac{t^{r-1}n^{r/2}\epsilon^{r}K_u^rH_{2r}(t)}{2^r} + n^{r}G_{r/2}K_u^r \epsilon^r t^{r/2 - 1}H_{r}(t)\right) \\
	 & \qquad\qquad\qquad\qquad + \int_0^t{3^{r-1}t^{r-1}K_u^r\avg{\norm{z_\tau^{(\epsilon)}(x) - z_\tau(x)}^r}\dif\tau}.
\end{align*}
Applying Gr\"{o}nwall's inequality, we have
\begin{align*}
	\avg{\norm{z_t^{(\epsilon)}(x) - z_t(x)}^r} % & \leq 3^{r-1}\left(\frac{t^{r-1}n^{r/2}\epsilon^{r}K_u^rH_{2r}(t)}{2^r}\right. \\
	% & \qquad\qquad\qquad\qquad \left.+ n^{r}K_u^r \epsilon^r t^{r/2 - 1}H_{r}(t)G_r\right)\exp\left(\int_0^t{3^{r - 1}t^{r-1}K_u^r\dif\tau}\right) \\
	 & = 3^{r-1}K_u^r\left(\frac{t^{r-1}n^{r/2}H_{2r}(t)}{2^r} + n^{r}G_{r/2}t^{r/2 - 1}H_{r}(t)\right)\exp\left(3^{r - 1}t^{r}K_u^r\right)\epsilon^r.
	% & = 3^{r-1}K_u^r\Bigg(\frac{t^{r-1}n^{r/2}2^{2r-1} n^{3r}K_\sigma^{2r} G_{r} t^{r + 1}\exp\left(2^{2r-1} K_u^{2r} t^{2r}\right)}{2^r}                                                                                                                                                 \\
	% & \qquad\qquad\qquad + n^{r}G_{r/2}t^{r/2 - 1}2^{r-1} n^{3r/2}K_\sigma^r G_{r/2} t^{r/2 + 1}\exp\left(2^{r-1} K_u^r t^r\right)\Bigg)\exp\left(3^{r - 1}t^{r}K_u^r\right)\epsilon^r                                                                                                    \\
	% & = 6^{r-1} K_u^r K_\sigma^r t^{r}\left(t^r n^{r/2}n^{3r}K_\sigma^{r} G_{r}\exp\left(2^{2r-1} K_u^{2r} t^{2r}\right) + n^{r}G_{r/2}^2n^{3r/2}\exp\left(2^{r-1} K_u^r t^r\right)\Bigg)\exp\left(3^{r - 1}t^{r}K_u^r\right)\epsilon^r\right)\exp\left(3^{r-1}t^r K_u^r\right)\epsilon^r
\end{align*}
Define
\begin{equation*}
	D_r(t) \coloneqq 3^{r-1}K_u^r\left(\frac{t^{r-1}n^{r/2}H_{2r}(t)}{2^r} + n^{r}G_{r/2}t^{r/2 - 1}H_{r}(t)\right)\exp\left(3^{r - 1}t^{r}K_u^r\right).
\end{equation*}
then we have shown \eqref{eqn:main_ineq}, the desired result.
% \[
% 	\avg{\norm{z_t^{(\epsilon)}(x) - z_t(x)}^r} \leq D_r(t)\epsilon^{r}.
% \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of \Cref{thm:gauss_dist}}\label{app:gauss_dist_proof}
In this appendix, we establish the exact Gaussian distribution of the solution to the linearised SDE.
\begin{lemma}\label{lem:z_lim_sol}
	The strong solution (e.g.\cite[Def. 6.1.1]{KallianpurSundar_2014_StochasticAnalysisDiffusion}) to the linearised SDE \eqref{eqn:limit_sde} can be written as the It\^o integral.
	\begin{equation}
		z_t(x) = \int_0^t{\nabla F_0^t(x)\left[\nabla F_0^\tau(x)\right]^{-1}\sigma\left(F_0^\tau(x), \tau\right)\dif W_\tau}.
		\label{eqn:z_ito_form}
	\end{equation}
\end{lemma}

\begin{proof}
	The equation \eqref{eqn:limit_sde} is linear and homogeneous, so the solution method is well-known, e.g. see \cite[\S4.3]{SarkkaSolin_2019_AppliedStochasticDifferential}.
	Let \(\Psi = \Psi(t)\) be the invertible fundamental matrix solution of the differential equation
	\begin{equation}
		\dod{\Psi(t)}{t} = \nabla u\left(F_0^t(x), t\right)\Psi(t), \quad \Psi(0) = I.
		\label{eqn:psi_eqn}
	\end{equation}
	The equation \eqref{eqn:psi_eqn} is exactly the equation of variations \eqref{eqn:eqn_of_vars} for the deterministic system, so \(\Psi(t) = \nabla F_0^t(x)\), noting that \(\Psi(0) = \nabla F_0^0(x) = \nabla x = I\).
	Using It\^o's Lemma (e.g. see \cite[Thm. 5.5.1]{KallianpurSundar_2014_StochasticAnalysisDiffusion}) on \(M(t) \coloneqq \Psi(t)^{-1}z_t(x)\), it follows that
	\[
		z_t(x) = \int_0^t{\nabla F_0^t(x)\left[\nabla F_0^\tau(x)\right]^{-1}\sigma\left(F_0^\tau(x), \tau\right)\dif W_\tau}
	\]
	satisfies \eqref{eqn:limit_sde} and is therefore a strong solution.
\end{proof}

We next establish that the It\^o integral of a matrix-valued deterministic function with respect to a multidimensional Wiener process is a multidimensional Gaussian process.
%This is a well-known fact in the scalar case, and has a straightforward extension.
\begin{lemma}\label{lem:det_gauss}
	Let \(a,b \in \R\) and let \(g: [a,b] \to \R^{n\times n}\) be a matrix-valued deterministic function such that each element of \(g\) is It\^o-integrable.
	Consider the It\^o integral
	\[
		\mathcal{I}[g] \coloneqq \int_{a}^b{g(t)\dif W_t},
	\]
	Then, the integral \(\mathcal{I}[g]\) is a \(n\)-dimensional multivariate Gaussian random variable.
\end{lemma}
\begin{proof}
	For \(i,j \in \set{1,\hdots,n}\), let \(g_{ij}: [a,b] \to \R\) be the \((i,j)\)th element of \(g\).
	Then, let
	\[
		\mathcal{I}[g_{ij}] \coloneqq \int_a^b{g_{ij}(t)\dif W_t^{(i)}},
	\]
	so that the \(i\)th element of \(\mathcal{I}[g]\) is
	\[
		\mathcal{I}[g]_i = \sum_{j = 1}^n{\mathcal{I}\left[g_{ij}\right]}.
	\]
	Each \(\mathcal{I}[g_{ij}]\) is an It\^o integral of a deterministic, scalar-valued function with respect to a one-dimensional Brownian motion, which is well-known to be a Gaussian process (e.g. see \cite[Lem. 4.3.11]{Applebaum_2004_LevyProcessesStochastic}).
	Moreover, each element of \(\mathcal{I}[g]\) is the sum of independent Gaussian random variables and is therefore itself Gaussian.
	Hence, \(\mathcal{I}[g]\) follows a multivariate Gaussian distribution.
\end{proof}


We can now prove \Cref{thm:gauss_dist}.
We have that
\[
	z_t(x) = \int_0^t{\nabla F_0^t(x)\left[\nabla F_0^\tau(x)\right]^{-1}\sigma\left(F_0^\tau(x), \tau\right)\dif W_\tau} = \int_0^t{L\left(x, t,\tau\right)\dif W_\tau},
\]
where \(L\) is as defined in \eqref{eqn:sigma_L_def}.
For any fixed \(t \in [0,T]\), \(z_t(x)\) is the It\^o integral of a deterministic, matrix-valued function, and so \(z_t(x)\) is an \(n\)-dimensional Gaussian random variable by \Cref{lem:det_gauss}.
Moreover \cite{KallianpurSundar_2014_StochasticAnalysisDiffusion}, \(\avg{z_t(x)} = 0\).
The variance of \(z_t(x)\) is then
\[
	\var{z_t(x)} = \avg{\left(z_t(x) - \avg{z_t(x)}\right)\left(z_t(x) - \avg{z_t(x)}\right)^{\T}} = \avg{z_t(x) z_t(x)^{\T}}.
\]
Let \(L_{ij}\) denote the \((i,j)\)th element of \(L\) for \(i,j \in \set{1,\hdots,n}\).
Then the \((i,j)\)th element of \(\var{z_t(x)}\) is
\begin{align*}
	\left[\var{z_t(x)}\right]_{ij} & = \avg{\left(\sum_{k=1}^n\int_0^t{L_{ik}\left(x, t, \tau\right)\dif W_{\tau}^{(k)}}\right)\left(\sum_{l=1}^n\int_0^t{L_{il}\left(x, t, \tau\right)\dif W_{\tau}^{(l)}}\right)}   \\
	                               & = \sum_{k=1}^{n}\sum_{l=1}^n\avg{\left(\int_0^t{L_{ik}\left(x, t, \tau\right)\dif W_{\tau}^{(k)}}\right)\left(\int_0^t{L_{il}\left(x, t, \tau\right)\dif W_{\tau}^{(l)}}\right)} \\
	% & = \sum_{k=1}^{n}\sum_{l=1}^n\delta_{kl}\avg{\int_0^t{L_{ik}\left(t, \tau\right)L_{jl}\left(t, \tau\right)\dif\tau}}                                                        \\
	                               & = \sum_{k=1}^n{\int_0^t{L_{ik}\left(x, t, \tau\right)L_{jk}\left(x, t, \tau\right)\dif\tau}}                                                                                     \\
	% & = \left[\int_0^t{L\left(x, t,\tau\right)L\left(x, t, \tau\right)^{\T}\dif \tau}\right]_{ij}                                                                                      \\
	                               & = \left[\Sigma(x,t)\right]_{ij}
\end{align*}
where the third equality uses It\^o's isometry \cite{KallianpurSundar_2014_StochasticAnalysisDiffusion} and the fact that \(W_t^{(k)}\) is independent of \(W_t^{(l)}\) for \(k \neq l\). % consists of independent components, where \(\delta_{kl}\) is the Kronecker delta.
So \(\var{z_t(x)} = \Sigma(x,t)\) and \(z_t(x) \isGauss{0, \Sigma(x,t)}\).
% Let \(L_{i\cdot}\) denote the \(i\)th row of \(L\), and for \(i \in \set{1,\hdots,n}\) define the scalar-valued stochastic process \(M^{(i)} = \set{M_t^{(i)}}_{t \in [0,T]}\) by
% \[
% 	M_t^{(i)} = \int_0^t{L_{i\cdot}\left(t, \tau\right)\dif W_\tau}.
% \]
% Then the \((i,j)\)th entry of \(\var{z_t}\) can be written as
% \begin{equation}
% 	\left[\var{z_t}\right]_{ij} = \avg{M_t^{(i)}M_t^{(j)}}.
% 	\label{eqn:z_var_exp}
% \end{equation}


% Now, for all \(t \in [0,T]\),
% \[
% 	\int_0^t{\norm{L_{i\cdot}\left(t,\tau\right)}^2\dif\tau} \leq \int_0^t{nK_F^4K_\sigma^2\dif\tau} < \infty,
% \]
% using the bounds under \ref{hyp:F_exists} and \ref{hyp:sigma_bounds}, so \(M\) is a continuous and square-integrable martingale.
% It then follows (e.g. see Section 5.3 of \cite{KallianpurSundar_2014_StochasticAnalysisDiffusion}) that for any \(i,j \in \set{1,\hdots,n}\), the stochastic process
% \[
% 	\set{M_t^{(i)}M_t^{(j)} - \int_0^t{L_{i\cdot}\left(t,\tau\right)L_{j\cdot}\left(t,\tau\right)^{\T}\dif\tau}}_{t \in [0,T]}
% \]
% is a martingale, where the integral term is the quadratic covariation of \(M^{(i)}\) and \(M^{(j)}\) over the interval \([0,t]\).
% By the martingale property, for any \(t \in [0,T]\)
% \[
% 	\avg{M_t^{(i)}M_t^{(j)} - \int_0^t{L_{i\cdot}\left(t,\tau\right)L_{j\cdot}\left(t,\tau\right)^{\T}\dif\tau}} = \avg{M_0^{(i)}M_0^{(j)} - \int_0^0{L_{i\cdot}\left(t,\tau\right)L_{j\cdot}\left(t,\tau\right)^{\T}\dif\tau}} = 0,
% \]
% so
% \[
% 	\avg{M_t^{(i)}M_t^{(j)}} = \avg{\int_0^t{L_{i\cdot}\left(t,\tau\right)L_{j\cdot}\left(t,\tau\right)^{\T}\dif\tau}} = \int_0^t{L_{i\cdot}\left(t,\tau\right)L_{j\cdot}\left(t,\tau\right)^{\T}\dif\tau}
% \]
% which is exactly the \((i,j)\)th element of
% \[
% 	\Sigma(t) = \int_0^t{L\left(t, \tau\right)L\left(t, \tau\right)^{\T}\dif\tau}.
% \]
Taking the limit as \(\epsilon \to 0\) in \eqref{eqn:main_ineq}, \(z_t^{(\epsilon)}(x)\) converges in \(r\)th moment to \(z_t(x)\), which implies that \(z_t^{(\epsilon)}(x)\) converges in distribution to \(z_t(x)\) also.
Since \(z_t(x)\) is known to follow a Gaussian distribution, we can write
\[
	z_t^{(\epsilon)}(x) \stackrel{d}{\longrightarrow} \mathcal{N}\left(0, \Sigma(x,t)\right).
\]

Finally, we show that \(\Sigma(x,t)\) is the solution to the matrix differential equation \eqref{eqn:sigma_ode}.
We clearly have that \(\Sigma(x,0) = O\), the \(n \times n\) zero matrix.
Differentiating the expression \eqref{eqn:sigma_calc} with the Leibniz integral rule
\begin{align*}
	\dod{\Sigma(x,t)}{t} & = L\left(x,t,t\right)L\left(x,t,t\right)^{\T} + \int_0^t{\dpd{}{t}\left[L\left(x,t,\tau\right)L\left(x,t,\tau\right)^{\T}\right]\dif\tau}          \\
	                     & = \sigma\left(F_0^t(x), t\right)\sigma\left(F_0^t(x), t\right)^{\T} + \int_0^t{\dpd{L\left(x,t,\tau\right)}{t}L\left(x,t,\tau\right)^{\T}\dif\tau} \\
	                     & \qquad\qquad\qquad\qquad + \int_0^t{L\left(x,t,\tau\right)\dpd{L\left(x,t,\tau\right)^{\T}}{t}\dif\tau}. \numberthis\label{eqn:sigma_deriv_expand}
\end{align*}
Now, using the equation of variations \eqref{eqn:eqn_of_vars},
\begin{equation}\label{eqn:L_deriv}
	\dpd{L\left(x,t,\tau\right)}{t} = \left[\nabla u\left(F_0^t(x), t\right)\right] L\left(x, t, \tau\right).
\end{equation}

Using \eqref{eqn:L_deriv} in \eqref{eqn:sigma_deriv_expand},
\begin{align*}
	\dod{\Sigma(x,t)}{t} & = \sigma\left(F_0^t(x), t\right)\sigma\left(F_0^t(x), t\right)^{\T} + \left[\nabla u\left(F_0^t(x), t\right)\right]\int_0^t{L\left(x,t,\tau\right)L\left(x,t,\tau\right)^{\T}\dif\tau}          \\
	                     & \qquad\qquad\qquad\qquad + \int_0^t{L\left(x,t,\tau\right)L\left(x,t,\tau\right)^{\T}\dif\tau}\left[\nabla u\left(F_0^t(x), t\right)\right]^{\T}                                                \\
	                     & = \sigma\left(F_0^t(x), t\right)\sigma\left(F_0^t(x), t\right)^{\T} + \left[\nabla u\left(F_0^t(x), t\right)\right]\Sigma(x,t) + \Sigma(x,t)\left[\nabla u\left(F_0^t(x), t\right)\right]^{\T}.
\end{align*}

% While limits in distribution are not unique, we can define a new Gaussian random variable \(\zeta_t\left(x\right)\) with
%	\[
%		\zeta_t(x) \isGauss{0, \Sigma(x,t)},
%	\]
%	so that \(z_t^{(\epsilon)}\left(x\right)\) converges in distribution to \(\zeta_t\left(x\right)\) also.

% \section{Proofs for stochastic sensitivity}

% \subsection{Consistency with original work}\label{app:s2_consistency}
% We first assert that the generalised definition of stochastic sensitivity in \Cref{def:ss_Rn} is consistent with that presented in \(\R^2\) in the original work \cite{Balasuriya_2020_StochasticSensitivityComputable}.
% In \cite{Balasuriya_2020_StochasticSensitivityComputable}, stochastic sensitivity is defined as
% \[
% 	S^2\left(x,t\right) \coloneqq \lim_{\epsilon\downarrow 0}\sup_{\theta \in \left[-\pi/2, \pi/2\right)}\var{\begin{pmatrix}
% 		\cos\theta \\ \sin\theta
% 	\end{pmatrix}^{\T}z^{(\epsilon)}_t(x)}.
% \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of \Cref{thm:s2_calculation}}\label{app:s2_calculation_proof}
Let \(x \in \R^n\) and \(t \in [0,T]\) be fixed.
We first establish that the \(\epsilon\)-limit and the variance operator can be exchanged.
Using the fact that \(\avg{z_t(x)} = 0\) and properties of expectation, we have
\[
	\norm{\avg{z_t^{(\epsilon)}(x)}} = \norm{\avg{z_t^{(\epsilon)}(x) - z_t(x)}} \leq \avg{\norm{z_t^{(\epsilon)}(x) - z_t(x)}} \leq D_1(t)\epsilon,
\]
so
\[
	\lim_{\epsilon\downarrow 0}\norm{\avg{z_t^{(\epsilon)}(x)}} = 0.
\]
Using the fact that \(\rho(z) = \norm{\avg{zz^{\T}}}^{1/2}\) defines a norm on the space of \(\R^n\)-valued random vectors and the reverse triangle inequality,
\begin{align*}
	\abs{\norm{\var{z_t^{(\epsilon)}(x)}}^{1/2} - \norm{\Sigma(x,t)}^{1/2}} & = \left|\norm{\avg{z_t^{(\epsilon)}(x)z_t^{(\epsilon)}(x)^{\T}} - \avg{z_t^{(\epsilon)}(x)}\avg{z_t^{(\epsilon)}(x)^{\T}}}^{1/2}\right. \\
	                                                                        & \qquad\qquad\qquad\qquad - \left.\norm{\avg{z_t(x) z_t(x)^{\T}}}^{1/2}\right|                                                           \\
	                                                                        & \leq \avg{\norm{z_t^{(\epsilon)}(x) - z_t(x)}^2}^{1/2} + \norm{\avg{z_t^{(\epsilon)}(x)}\avg{z_t^{(\epsilon)}(x)}^{\T}}^{1/2}           \\
	                                                                        & \leq D_2(t)^{1/2}\epsilon + \norm{\avg{z_t^{(\epsilon)}(x)}},
\end{align*}
and so taking the limit as \(\epsilon\downarrow 0\) and squaring both sides, we have
\begin{equation}
	\lim_{\epsilon\downarrow 0}\norm{\var{z_t^{(\epsilon)}(x)}} = \norm{\Sigma(x,t)}.
	\label{eqn:sigma_lim}
\end{equation}
For \(\epsilon > 0\), define
\[
	S^2_{(\epsilon)}(x,t) \coloneqq \sup{\setc{\var{p^{\T}z_t^{(\epsilon)}(x)}}{p \in \R^n, \, \norm{p} = 1}} = \sup{\setc{p^{\T}\var{z_t^{(\epsilon)}(x)}p}{p \in \R^n, \, \norm{p} = 1}}
\]
Since \(\var{z_t^{(\epsilon)}(x)}\) is symmetric and positive definite, the Cholesky decomposition provides an \(n \times n\) matrix \(\Pi^{(\epsilon)}\left(x,t\right)\) such that
\[
	\var{z_t^{(\epsilon)}(x)} = \left[\Pi^{(\epsilon)}(x,t)\right]^{\T}\Pi^{(\epsilon)}(x,t),
\]
allowing us to write
\begin{align*}
	S^2_{(\epsilon)}(x,t) % & = \sup\setc{p^{\T}\left[\Pi^{(\epsilon)}(x,t)\right]^{\T}\Pi^{(\epsilon)}(x,t)p}{p \in \R^n, \, \norm{p} = 1} \\
	% & = \sup\setc{\norm{\Pi^{(\epsilon)}(x,t)p}^2}{p \in \R^n, \, \norm{p} = 1}                                       \\
	 & = \norm{\Pi^{(\epsilon)}(x,t)}^2 = \norm{\var{z_t^{(\epsilon)}(x)}}.
\end{align*}
using properties of the spectral norm.
Taking the limit as \(\epsilon\) approaches zero and using \eqref{eqn:sigma_lim},
\[
	S^2(x,t) = \lim_{\epsilon\downarrow 0} S^2_{(\epsilon)}(x,t) = \lim_{\epsilon\downarrow 0}\norm{\var{z_t^{(\epsilon)}(x)}} = \norm{\Sigma(x,t)}.
\]
Since \(\Sigma(x,t)\) is symmetric and positive definite, the operator norm, and therefore \(S^2(x,t)\), is given by the largest eigenvalue of \(\Sigma(x,t)\).
